{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-30T18:40:20.764481Z",
     "iopub.status.busy": "2025-11-30T18:40:20.764160Z",
     "iopub.status.idle": "2025-11-30T19:20:15.043201Z",
     "shell.execute_reply": "2025-11-30T19:20:15.042534Z",
     "shell.execute_reply.started": "2025-11-30T18:40:20.764457Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: kaggle_evaluation not found. Если ты запускаешь скрипт локально (не на Kaggle), это ожидаемо: сервер инференса и автогенерация submission.csv работать не будут.\n",
      "======================================================================\n",
      "NFL BIG DATA BOWL 2026 - LGBM + XGB + GNN (rich features, Eval API)\n",
      "======================================================================\n",
      "DEVICE: cuda\n",
      "\n",
      "[1/4] Loading training inputs/outputs...\n",
      " Week 01: input (285714, 23), output (32088, 6)\n",
      " Week 02: input (288586, 23), output (32180, 6)\n",
      " Week 03: input (297757, 23), output (36080, 6)\n",
      " Week 04: input (272475, 23), output (30147, 6)\n",
      " Week 05: input (254779, 23), output (29319, 6)\n",
      " Week 06: input (270676, 23), output (31162, 6)\n",
      " Week 07: input (233597, 23), output (27443, 6)\n",
      " Week 08: input (281011, 23), output (33017, 6)\n",
      " Week 09: input (252796, 23), output (28291, 6)\n",
      " Week 10: input (260372, 23), output (29008, 6)\n",
      " Week 11: input (243413, 23), output (27623, 6)\n",
      " Week 12: input (294940, 23), output (32156, 6)\n",
      " Week 13: input (233755, 23), output (29568, 6)\n",
      " Week 14: input (279972, 23), output (32873, 6)\n",
      " Week 15: input (281820, 23), output (32715, 6)\n",
      " Week 16: input (316417, 23), output (36508, 6)\n",
      " Week 17: input (277582, 23), output (33076, 6)\n",
      " Week 18: input (254917, 23), output (29682, 6)\n",
      "Train inputs: (4880579, 23) train outputs: (562936, 6)\n",
      "\n",
      "[2/4] Preparing training features...\n",
      " Filtered to player_to_predict==True: 562936 -> 562936 rows\n",
      " After symmetry augmentation (tree): 562936 -> 1125872 rows\n",
      "\n",
      "[3/4] Training models...\n",
      "\n",
      "[2.5] Light hyperparameter tuning for LGBM...\n",
      "  Subsampled train_tree: 1125872 -> 300000 rows for tuning\n",
      "  Trying LGBM params: {'num_leaves': 63, 'learning_rate': 0.05, 'n_estimators': 800}\n",
      "    -> val RMSE: 0.7855\n",
      "  Trying LGBM params: {'num_leaves': 127, 'learning_rate': 0.05, 'n_estimators': 1000}\n",
      "    -> val RMSE: 0.6087\n",
      "  Trying LGBM params: {'num_leaves': 127, 'learning_rate': 0.07, 'n_estimators': 800}\n",
      "    -> val RMSE: 0.6034\n",
      "  Trying LGBM params: {'num_leaves': 255, 'learning_rate': 0.05, 'n_estimators': 1200}\n",
      "    -> val RMSE: 0.4892\n",
      "  Trying LGBM params: {'num_leaves': 255, 'learning_rate': 0.03, 'n_estimators': 1600}\n",
      "    -> val RMSE: 0.5047\n",
      "  Trying LGBM params: {'num_leaves': 63, 'learning_rate': 0.03, 'n_estimators': 1400}\n",
      "    -> val RMSE: 0.7770\n",
      "  Best LGBM params: {'num_leaves': 255, 'learning_rate': 0.05, 'n_estimators': 1200}, val RMSE: 0.4892\n",
      "\n",
      "[2.6] Light hyperparameter tuning for XGBoost...\n",
      "  Subsampled train_tree: 1125872 -> 300000 rows for tuning\n",
      "  Trying XGB params: {'max_depth': 6, 'learning_rate': 0.05, 'n_estimators': 400, 'subsample': 0.9, 'colsample_bytree': 0.9}\n",
      "    -> val RMSE: 1.0294\n",
      "  Trying XGB params: {'max_depth': 8, 'learning_rate': 0.05, 'n_estimators': 600, 'subsample': 0.9, 'colsample_bytree': 0.9}\n",
      "    -> val RMSE: 0.7082\n",
      "  Trying XGB params: {'max_depth': 8, 'learning_rate': 0.07, 'n_estimators': 500, 'subsample': 0.9, 'colsample_bytree': 0.9}\n",
      "    -> val RMSE: 0.6909\n",
      "  Trying XGB params: {'max_depth': 10, 'learning_rate': 0.05, 'n_estimators': 700, 'subsample': 0.85, 'colsample_bytree': 0.8}\n",
      "    -> val RMSE: 0.5155\n",
      "  Trying XGB params: {'max_depth': 6, 'learning_rate': 0.03, 'n_estimators': 800, 'subsample': 0.9, 'colsample_bytree': 0.9}\n",
      "    -> val RMSE: 0.9779\n",
      "  Best XGB params: {'max_depth': 10, 'learning_rate': 0.05, 'n_estimators': 700, 'subsample': 0.85, 'colsample_bytree': 0.8}, val RMSE: 0.5155\n",
      "\n",
      "  Training LGBM ensemble with tuned params: {'objective': 'regression', 'boosting_type': 'gbdt', 'n_estimators': 1200, 'learning_rate': 0.05, 'num_leaves': 255, 'min_data_in_leaf': 50, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1, 'verbosity': -1}\n",
      "   -> LGBM model 1/3 for dx...\n",
      "   -> LGBM model 1/3 for dy...\n",
      "   -> LGBM model 2/3 for dx...\n",
      "   -> LGBM model 2/3 for dy...\n",
      "   -> LGBM model 3/3 for dx...\n",
      "   -> LGBM model 3/3 for dy...\n",
      "✓ LGBM ensemble trained on full dataset\n",
      "\n",
      "  Training XGBoost models with tuned params: {'objective': 'reg:squarederror', 'tree_method': 'hist', 'enable_categorical': True, 'n_jobs': -1, 'reg_lambda': 1.0, 'random_state': 42, 'max_depth': 10, 'learning_rate': 0.05, 'n_estimators': 700, 'subsample': 0.85, 'colsample_bytree': 0.8}\n",
      "   -> XGB model for dx...\n",
      "   -> XGB model for dy...\n",
      "✓ XGBoost models trained on full dataset\n",
      " Training GAT+GCN model (dx, dy)...\n",
      "  GNN epoch 1/5: loss = 1.816444\n",
      "  GNN epoch 2/5: loss = 1.157725\n",
      "  GNN epoch 3/5: loss = 1.064329\n",
      "  GNN epoch 4/5: loss = 0.976462\n",
      "  GNN epoch 5/5: loss = 0.929466\n",
      "✓ GNN model trained\n",
      "\n",
      "[4/4] Tuning ensemble weights (LGBM / XGB / GNN)...\n",
      "  Subsampled train_tree: 1125872 -> 50000 rows for ensemble tuning\n",
      "  Processed 42913 graphs for GNN predictions\n",
      "  Single-model RMSE [LGBM] = 0.2958\n",
      "  Single-model RMSE [XGB] = 0.3349\n",
      "  Single-model RMSE [GNN] = 3.9251\n",
      "  Searching weights for 3-model ensemble (LGBM+XGB+GNN) with step=0.1 ...\n",
      "  Best ensemble weights: LGBM=0.80, XGB=0.20, GNN=-0.00, RMSE=0.2931\n",
      "  Saved 3 LGBM dx and 3 LGBM dy models to models\n",
      "  Saved 1 XGB dx and 1 XGB dy models to models\n",
      "  Saved GNN model state_dict to gnn_model.pth\n",
      "  Saved metadata to meta.pkl\n",
      "\n",
      "NFLInferenceServer недоступен (kaggle_evaluation не найден). Локально можно тренировать модель и дебажить фичи, но для генерации submission.csv нужно запускать код в Kaggle с подключённым датасетом 'nfl-big-data-bowl-2026-prediction' во вкладке Data.\n"
     ]
    }
   ],
   "source": [
    "# ======================== IMPORTS & CONFIG ========================\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from lightgbm import LGBMRegressor\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = \"/kaggle/input/nfl-big-data-bowl-2026-prediction\"\n",
    "\n",
    "# Inference server setup\n",
    "sys.path.append(DATA_DIR)\n",
    "try:\n",
    "    from kaggle_evaluation.nfl_inference_server import NFLInferenceServer\n",
    "    HAS_EVAL_SERVER = True\n",
    "except ModuleNotFoundError:\n",
    "    NFLInferenceServer = None\n",
    "    HAS_EVAL_SERVER = False\n",
    "    print(\n",
    "        \"WARNING: kaggle_evaluation not found. \"\n",
    "  \n",
    "    )\n",
    "\n",
    "# Random seeds\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"NFL BIG DATA BOWL 2026 - LGBM + XGB + GNN (rich features, Eval API)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "# ======================== FEATURE LISTS ===========================\n",
    "FEATURES = [\n",
    "     # geometry of the current state\n",
    "    \"x_last\", \"y_last\",\n",
    "    \"s\", \"a\", \"o\", \"dir\",\n",
    "\n",
    "    # components of speed and acceleration along the field axes\n",
    "\n",
    "    \"vx\", \"vy\",\n",
    "    \"ax_comp\", \"ay_comp\",\n",
    "    \"dir_sin\", \"dir_cos\",\n",
    "    \"o_sin\", \"o_cos\",\n",
    "\n",
    "    # temporal features\n",
    "\n",
    "    \"frame_offset\", \"time_offset\",\n",
    "    \"num_frames_output\",\n",
    "    \"frac_of_flight\",\n",
    "    \"frames_left\",\n",
    "    \"time_to_land\",\n",
    "    \"remaining_flight_frac\",\n",
    "\n",
    "    # relation to the ball landing point\n",
    "\n",
    "    \"dist_to_ball_land\",\n",
    "    \"angle_to_ball_land\",\n",
    "    \"dist_to_ball_land_per_frame\",\n",
    "    \"cos_dir_to_ball\",\n",
    "    \"cos_orient_to_ball\",\n",
    "\n",
    "    # relative coordinates and velocity projections toward the ball\n",
    "\n",
    "    \"x_rel_ball\",\n",
    "    \"y_rel_ball\",\n",
    "    \"v_toward_ball\",\n",
    "    \"v_across_ball\",\n",
    "\n",
    "    # standardized by play direction along x\n",
    "\n",
    "    \"x_std\",\n",
    "    \"ball_land_x_std\",\n",
    "    \"dx_to_land_std\",\n",
    "    \"dy_to_land\",\n",
    "\n",
    "    # position by field width and length\n",
    "\n",
    "    \"dist_to_sideline\",\n",
    "    \"dist_to_center\",\n",
    "    \"yardline_100\",\n",
    "    \"yardline_norm\",\n",
    "    \"dist_to_endzone\",\n",
    "\n",
    "    # target receiver and velocity projections toward him\n",
    "    \"dist_to_target_last\",\n",
    "    \"dx_to_target_last\",\n",
    "    \"dy_to_target_last\",\n",
    "    \"angle_to_target\",\n",
    "    \"cos_dir_to_target\",\n",
    "    \"cos_orient_to_target\",\n",
    "    \"v_toward_target\",\n",
    "    \"v_across_target\",\n",
    "    \"is_target\",\n",
    "\n",
    "    # play / player context\n",
    "\n",
    "    \"absolute_yardline_number\",\n",
    "    \"player_height\", \"player_weight\",\n",
    "    \"bmi\",\n",
    "\n",
    "    # pairwise context (approximation of attention/GNN in tabular form)\n",
    "\n",
    "    \"min_dist_teammate\",\n",
    "    \"mean_dist_teammate\",\n",
    "    \"min_dist_opponent\",\n",
    "    \"mean_dist_opponent\",\n",
    "]\n",
    "\n",
    "CAT_FEATS = [\"player_role\", \"player_side\", \"play_direction\"]\n",
    "\n",
    "## For GNN we use the same features\n",
    "\n",
    "GNN_NUM_FEATS = FEATURES\n",
    "GNN_CAT_CODE_COLS = [f\"{c}_cat\" for c in CAT_FEATS]\n",
    "\n",
    "## Which columns we pull from \"the last observation before the pass\"\n",
    "\n",
    "BASE_COLS = [\n",
    "    \"game_id\", \"play_id\", \"nfl_id\",\n",
    "    \"x_last\", \"y_last\",\n",
    "    \"s\", \"a\", \"o\", \"dir\",\n",
    "    \"player_role\", \"player_side\",\n",
    "    \"num_frames_output\",\n",
    "    \"ball_land_x\", \"ball_land_y\",\n",
    "    \"target_last_x\", \"target_last_y\", \"target_nfl_id\",\n",
    "    \"play_direction\",\n",
    "    \"absolute_yardline_number\",\n",
    "    \"player_height\", \"player_weight\",\n",
    "    \"player_to_predict\",\n",
    "    # pairwise features from last_obs\n",
    "    \"min_dist_teammate\",\n",
    "    \"mean_dist_teammate\",\n",
    "    \"min_dist_opponent\",\n",
    "    \"mean_dist_opponent\",\n",
    "]\n",
    "\n",
    "# ======================== GLOBAL STATE ============================\n",
    "# LGBM ensemble\n",
    "LGBM_MODELS_DX = []\n",
    "LGBM_MODELS_DY = []\n",
    "\n",
    "# XGBoost\n",
    "XGB_MODELS_DX = []\n",
    "XGB_MODELS_DY = []\n",
    "\n",
    "# GNN \n",
    "GNN_MODEL = None\n",
    "\n",
    "# Categorical dictionaries for embeddings/encoding\n",
    "\n",
    "CAT_CATEGORY_MAPS = {}\n",
    "CAT_CARD_SIZES = {}\n",
    "\n",
    "# Normalization of numerical features for GNN\n",
    "GNN_NUM_MEAN = None\n",
    "GNN_NUM_STD = None\n",
    "\n",
    "# ВTarget-player weights in training\n",
    "\n",
    "TARGET_WEIGHT_TREE = 1.0   # LGBM/XGB weight for target receiver\n",
    "\n",
    "TARGET_WEIGHT_GNN = 1.0    # GNN weight for target receiver\n",
    "\n",
    "# LGBM ensemble size\n",
    "LGBM_N_MODELS = 3\n",
    "\n",
    "# GNN hyperparameters\n",
    "\n",
    "TRAIN_GNN = True           # can set False to speed up training since GNN has not prooved to help our approach\n",
    "GNN_EPOCHS = 5\n",
    "GNN_BATCH_SIZE = 512\n",
    "GNN_HIDDEN_DIM = 256\n",
    "GNN_EMB_DIM = 8\n",
    "GNN_GAT_HEADS = 2\n",
    "GNN_LR = 1e-3\n",
    "GNN_DROPOUT = 0.2\n",
    "\n",
    "# Weights for ensemble (LGBM / XGB / GNN): These were used for final predictions. In otherwords this is only LightGBM\n",
    "ENSEMBLE_WEIGHTS = {\"lgbm\": 1.0, \"xgb\": 0.0, \"gnn\": 0.0}\n",
    "\n",
    "# ======================== DATA LOADING ===========================\n",
    "def load_train(data_dir: str):\n",
    "    \"\"\"train input/output.\"\"\"\n",
    "    train_dir = os.path.join(data_dir, \"train\")\n",
    "    df_in_list = []\n",
    "    df_out_list = []\n",
    "\n",
    "    print(\"\\n[1/4] Loading training inputs/outputs...\")\n",
    "    for w in range(1, 19):\n",
    "        ip = os.path.join(train_dir, f\"input_2023_w{w:02d}.csv\")\n",
    "        op = os.path.join(train_dir, f\"output_2023_w{w:02d}.csv\")\n",
    "        if os.path.exists(ip) and os.path.exists(op):\n",
    "            df_i = pd.read_csv(ip)\n",
    "            df_o = pd.read_csv(op)\n",
    "            df_in_list.append(df_i)\n",
    "            df_out_list.append(df_o)\n",
    "            print(f\" Week {w:02d}: input {df_i.shape}, output {df_o.shape}\")\n",
    "        else:\n",
    "            print(f\" Week {w:02d}: files not found, skipping\")\n",
    "\n",
    "    if not df_in_list or not df_out_list:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No train CSV files found in {train_dir}. \"\n",
    "        )\n",
    "\n",
    "    df_in = pd.concat(df_in_list, ignore_index=True)\n",
    "    df_out = pd.concat(df_out_list, ignore_index=True)\n",
    "    print(\"Train inputs:\", df_in.shape, \"train outputs:\", df_out.shape)\n",
    "    return df_in, df_out\n",
    "\n",
    "\n",
    "# ======================== FEATURE ENGINEERING HELPERS =============\n",
    "def height_to_inches(ht):\n",
    "    \"\"\"Convert height from the format '6-2' into inches (6*12 + 2).\"\"\"\n",
    "    if isinstance(ht, str) and \"-\" in ht:\n",
    "        try:\n",
    "            feet, inches = ht.split(\"-\")\n",
    "            return int(feet) * 12 + int(inches)\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def add_team_distance_features(df_last: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pairwise features for players within (game_id, play_id):\n",
    "    - minimum/mean distance to teammates,\n",
    "    - minimum/mean distance to opponents.\n",
    "\n",
    "    \"\"\"\n",
    "    if \"player_side\" not in df_last.columns:\n",
    "        df_last[\"min_dist_teammate\"] = 0.0\n",
    "        df_last[\"mean_dist_teammate\"] = 0.0\n",
    "        df_last[\"min_dist_opponent\"] = 0.0\n",
    "        df_last[\"mean_dist_opponent\"] = 0.0\n",
    "        return df_last\n",
    "\n",
    "    groups = []\n",
    "    for (_, _), g in df_last.groupby([\"game_id\", \"play_id\"], as_index=False):\n",
    "        g = g.copy()\n",
    "        xs = g[\"x_last\"].to_numpy()\n",
    "        ys = g[\"y_last\"].to_numpy()\n",
    "        sides = g[\"player_side\"].astype(\"category\").cat.codes.to_numpy()\n",
    "\n",
    "        dx = xs[:, None] - xs[None, :]\n",
    "        dy = ys[:, None] - ys[None, :]\n",
    "        dist = np.sqrt(dx * dx + dy * dy)\n",
    "        np.fill_diagonal(dist, np.inf)\n",
    "\n",
    "        same = sides[:, None] == sides[None, :]\n",
    "        opp = ~same\n",
    "\n",
    "        dist_tm = np.where(same, dist, np.inf)\n",
    "        min_dist_tm = dist_tm.min(axis=1)\n",
    "        min_dist_tm[np.isinf(min_dist_tm)] = 0.0\n",
    "\n",
    "        sum_tm = np.where(same, dist, 0.0).sum(axis=1)\n",
    "        cnt_tm = same.sum(axis=1) - 1\n",
    "        mean_tm = np.divide(\n",
    "            sum_tm,\n",
    "            np.maximum(cnt_tm, 1),\n",
    "            out=np.zeros_like(sum_tm),\n",
    "            where=cnt_tm > 0,\n",
    "        )\n",
    "\n",
    "        dist_op = np.where(opp, dist, np.inf)\n",
    "        min_dist_op = dist_op.min(axis=1)\n",
    "        min_dist_op[np.isinf(min_dist_op)] = 0.0\n",
    "\n",
    "        sum_op = np.where(opp, dist, 0.0).sum(axis=1)\n",
    "        cnt_op = opp.sum(axis=1)\n",
    "        mean_op = np.divide(\n",
    "            sum_op,\n",
    "            np.maximum(cnt_op, 1),\n",
    "            out=np.zeros_like(sum_op),\n",
    "            where=cnt_op > 0,\n",
    "        )\n",
    "\n",
    "        g[\"min_dist_teammate\"] = min_dist_tm\n",
    "        g[\"mean_dist_teammate\"] = mean_tm\n",
    "        g[\"min_dist_opponent\"] = min_dist_op\n",
    "        g[\"mean_dist_opponent\"] = mean_op\n",
    "\n",
    "        groups.append(g)\n",
    "\n",
    "    return pd.concat(groups, ignore_index=True)\n",
    "\n",
    "\n",
    "def prepare_last_obs(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Take the last observation by (game_id, play_id, nfl_id)  \n",
    "    and rename x,y -> x_last, y_last. Also convert height  \n",
    "    and add pairwise features.\n",
    "    \"\"\"\n",
    "    df_last = (\n",
    "        df.sort_values([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n",
    "          .groupby([\"game_id\", \"play_id\", \"nfl_id\"], as_index=False)\n",
    "          .last()\n",
    "    )\n",
    "    df_last = df_last.rename(columns={\"x\": \"x_last\", \"y\": \"y_last\"})\n",
    "\n",
    "    if \"player_height\" in df_last.columns:\n",
    "        df_last[\"player_height\"] = df_last[\"player_height\"].apply(height_to_inches)\n",
    "    else:\n",
    "        df_last[\"player_height\"] = np.nan\n",
    "\n",
    "    df_last = add_team_distance_features(df_last)\n",
    "    return df_last\n",
    "\n",
    "\n",
    "def add_target_info(df_last: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"For each play, add the coordinates of the target receiver.\"\"\"\n",
    "    mask_target = df_last.get(\"player_role\", \"\") == \"Targeted Receiver\"\n",
    "    targets = df_last.loc[\n",
    "        mask_target,\n",
    "        [\"game_id\", \"play_id\", \"nfl_id\", \"x_last\", \"y_last\"],\n",
    "    ].copy()\n",
    "\n",
    "    targets = targets.rename(\n",
    "        columns={\n",
    "            \"nfl_id\": \"target_nfl_id\",\n",
    "            \"x_last\": \"target_last_x\",\n",
    "            \"y_last\": \"target_last_y\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df_last = df_last.merge(\n",
    "        targets[[\"game_id\", \"play_id\", \"target_last_x\", \"target_last_y\", \"target_nfl_id\"]],\n",
    "        on=[\"game_id\", \"play_id\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    return df_last\n",
    "\n",
    "\n",
    "def mirror_raw(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Mirror the play along the Y-axis\n",
    "\n",
    "    \"\"\"\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    if \"y_last\" in df.columns:\n",
    "        df[\"y_last\"] = 53.3 - df[\"y_last\"]\n",
    "    if \"y\" in df.columns:\n",
    "        df[\"y\"] = 53.3 - df[\"y\"]\n",
    "    if \"ball_land_y\" in df.columns:\n",
    "        df[\"ball_land_y\"] = 53.3 - df[\"ball_land_y\"]\n",
    "    if \"target_last_y\" in df.columns:\n",
    "        df[\"target_last_y\"] = 53.3 - df[\"target_last_y\"]\n",
    "\n",
    "    for ang_col in [\"dir\", \"o\"]:\n",
    "        if ang_col in df.columns:\n",
    "            df[ang_col] = (-df[ang_col]) % 360.0\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_features(df: pd.DataFrame, is_train: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build features (as in your dataset), plus dx, dy if is_train=True.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # -------- Basic quantities and angles --------\n",
    "    s = df[\"s\"].fillna(0.0)\n",
    "    a = df[\"a\"].fillna(0.0)\n",
    "    dir_rad = np.deg2rad(df[\"dir\"].fillna(0.0))\n",
    "    o_rad = np.deg2rad(df[\"o\"].fillna(0.0))\n",
    "\n",
    "    # -------- Components of speed and acceleration --------\n",
    "    df[\"vx\"] = s * np.cos(dir_rad)\n",
    "    df[\"vy\"] = s * np.sin(dir_rad)\n",
    "    df[\"ax_comp\"] = a * np.cos(dir_rad)\n",
    "    df[\"ay_comp\"] = a * np.sin(dir_rad)\n",
    "\n",
    "    df[\"dir_sin\"] = np.sin(dir_rad)\n",
    "    df[\"dir_cos\"] = np.cos(dir_rad)\n",
    "    df[\"o_sin\"] = np.sin(o_rad)\n",
    "    df[\"o_cos\"] = np.cos(o_rad)\n",
    "\n",
    "    # -------- Flight time / frame --------\n",
    "    if \"frame_id\" in df.columns:\n",
    "        df[\"frame_offset\"] = df[\"frame_id\"]\n",
    "    else:\n",
    "        df[\"frame_offset\"] = 0\n",
    "\n",
    "    df[\"time_offset\"] = df[\"frame_offset\"] / 10.0  # 10 frames per second\n",
    "\n",
    "    if \"num_frames_output\" in df.columns:\n",
    "        nfo = df[\"num_frames_output\"].replace(0, np.nan)\n",
    "        df[\"frac_of_flight\"] = (df[\"frame_offset\"] / nfo).clip(lower=0, upper=1)\n",
    "        df[\"frac_of_flight\"] = df[\"frac_of_flight\"].fillna(0.0)\n",
    "        df[\"frames_left\"] = (nfo - df[\"frame_offset\"]).clip(lower=0).fillna(0.0)\n",
    "    else:\n",
    "        df[\"frac_of_flight\"] = 0.0\n",
    "        df[\"frames_left\"] = 0.0\n",
    "\n",
    "    df[\"time_to_land\"] = df[\"frames_left\"] / 10.0\n",
    "    df[\"remaining_flight_frac\"] = (1.0 - df[\"frac_of_flight\"]).clip(lower=0.0, upper=1.0)\n",
    "\n",
    "    # -------- Geometry relative to the ball landing point --------\n",
    "    df[\"dist_to_ball_land\"] = np.sqrt(\n",
    "        (df[\"ball_land_x\"] - df[\"x_last\"]) ** 2 +\n",
    "        (df[\"ball_land_y\"] - df[\"y_last\"]) ** 2\n",
    "    )\n",
    "    df[\"angle_to_ball_land\"] = np.arctan2(\n",
    "        df[\"ball_land_y\"] - df[\"y_last\"],\n",
    "        df[\"ball_land_x\"] - df[\"x_last\"],\n",
    "    )\n",
    "\n",
    "    frames_left_safe = df[\"frames_left\"].replace(0, np.nan)\n",
    "    df[\"dist_to_ball_land_per_frame\"] = df[\"dist_to_ball_land\"] / frames_left_safe\n",
    "    df[\"dist_to_ball_land_per_frame\"] = (\n",
    "        df[\"dist_to_ball_land_per_frame\"]\n",
    "        .replace([np.inf, -np.inf], np.nan)\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "\n",
    "    df[\"cos_dir_to_ball\"] = np.cos(df[\"angle_to_ball_land\"] - dir_rad)\n",
    "    df[\"cos_orient_to_ball\"] = np.cos(df[\"angle_to_ball_land\"] - o_rad)\n",
    "\n",
    "    # -------- Standardize coordinates by the direction of the play --------\n",
    "    play_dir = df.get(\"play_direction\", \"right\").fillna(\"right\")\n",
    "    is_left = (play_dir == \"left\").astype(int)\n",
    "\n",
    "    df[\"x_std\"] = np.where(is_left == 1, 120.0 - df[\"x_last\"], df[\"x_last\"])\n",
    "    df[\"ball_land_x_std\"] = np.where(\n",
    "        is_left == 1, 120.0 - df[\"ball_land_x\"], df[\"ball_land_x\"]\n",
    "    )\n",
    "\n",
    "    df[\"dx_to_land_std\"] = df[\"ball_land_x_std\"] - df[\"x_std\"]\n",
    "    df[\"dy_to_land\"] = df[\"ball_land_y\"] - df[\"y_last\"]\n",
    "\n",
    "    # -------- Position by field width/length --------\n",
    "    df[\"dist_to_sideline\"] = np.minimum(df[\"y_last\"], 53.3 - df[\"y_last\"])\n",
    "    df[\"dist_to_center\"] = np.abs(df[\"y_last\"] - 53.3 / 2.0)\n",
    "\n",
    "    yard = df[\"absolute_yardline_number\"].fillna(50.0)\n",
    "    yard_100 = yard.clip(lower=0.0, upper=100.0)\n",
    "    df[\"yardline_100\"] = yard_100\n",
    "    df[\"yardline_norm\"] = yard_100 / 100.0\n",
    "    df[\"dist_to_endzone\"] = 100.0 - yard_100\n",
    "\n",
    "    # -------- Target receiver --------\n",
    "    df[\"dist_to_target_last\"] = np.sqrt(\n",
    "        (df[\"target_last_x\"] - df[\"x_last\"]) ** 2 +\n",
    "        (df[\"target_last_y\"] - df[\"y_last\"]) ** 2\n",
    "    )\n",
    "\n",
    "    df[\"dx_to_target_last\"] = df[\"target_last_x\"] - df[\"x_last\"]\n",
    "    df[\"dy_to_target_last\"] = df[\"target_last_y\"] - df[\"y_last\"]\n",
    "    df[\"angle_to_target\"] = np.arctan2(\n",
    "        df[\"target_last_y\"] - df[\"y_last\"],\n",
    "        df[\"target_last_x\"] - df[\"x_last\"],\n",
    "    )\n",
    "\n",
    "    df[\"cos_dir_to_target\"] = np.cos(df[\"angle_to_target\"] - dir_rad)\n",
    "    df[\"cos_orient_to_target\"] = np.cos(df[\"angle_to_target\"] - o_rad)\n",
    "\n",
    "    df[\"is_target\"] = (df[\"nfl_id\"] == df[\"target_nfl_id\"]).astype(int)\n",
    "\n",
    "    # -------- Relative coordinates and velocity projections --------\n",
    "    df[\"x_rel_ball\"] = df[\"x_last\"] - df[\"ball_land_x\"]\n",
    "    df[\"y_rel_ball\"] = df[\"y_last\"] - df[\"ball_land_y\"]\n",
    "\n",
    "    vx = df[\"vx\"]\n",
    "    vy = df[\"vy\"]\n",
    "\n",
    "    ball_cos = np.cos(df[\"angle_to_ball_land\"])\n",
    "    ball_sin = np.sin(df[\"angle_to_ball_land\"])\n",
    "    df[\"v_toward_ball\"] = vx * ball_cos + vy * ball_sin\n",
    "    df[\"v_across_ball\"] = vx * (-ball_sin) + vy * ball_cos\n",
    "\n",
    "    tgt_cos = np.cos(df[\"angle_to_target\"])\n",
    "    tgt_sin = np.sin(df[\"angle_to_target\"])\n",
    "    df[\"v_toward_target\"] = vx * tgt_cos + vy * tgt_sin\n",
    "    df[\"v_across_target\"] = vx * (-tgt_sin) + vy * tgt_cos\n",
    "\n",
    "    df[[\"v_toward_ball\", \"v_across_ball\", \"v_toward_target\", \"v_across_target\"]] = (\n",
    "        df[[\"v_toward_ball\", \"v_across_ball\", \"v_toward_target\", \"v_across_target\"]]\n",
    "        .replace([np.inf, -np.inf], np.nan)\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "\n",
    "    # -------- Player physics --------\n",
    "    h = df[\"player_height\"].replace(0, np.nan)\n",
    "    w = df[\"player_weight\"].replace(0, np.nan)\n",
    "    df[\"bmi\"] = 703.0 * (w / (h ** 2))\n",
    "    df[\"bmi\"] = df[\"bmi\"].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    \n",
    "    # -------- Targets only in train --------\n",
    "    if is_train:\n",
    "        df[\"dx\"] = df[\"x\"] - df[\"x_last\"]\n",
    "        df[\"dy\"] = df[\"y\"] - df[\"y_last\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_train(df_in: pd.DataFrame, df_out: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Form training datasets:\n",
    "    - train_tree: with symmetric augmentation (for LGBM/XGB),\n",
    "    - train_gnn: without augmentation (for GNN).\n",
    "    \"\"\"\n",
    "    print(\"\\n[2/4] Preparing training features...\")\n",
    "\n",
    "    df_out_local = df_out.copy()\n",
    "    if \"frame_id\" not in df_out_local.columns:\n",
    "        df_out_local[\"frame_id\"] = (\n",
    "            df_out_local.groupby([\"game_id\", \"play_id\", \"nfl_id\"]).cumcount()\n",
    "        )\n",
    "\n",
    "    last_obs = prepare_last_obs(df_in)\n",
    "    last_obs = add_target_info(last_obs)\n",
    "\n",
    "    cols_to_keep_existing = [c for c in BASE_COLS if c in last_obs.columns]\n",
    "\n",
    "    train_raw = df_out_local.merge(\n",
    "        last_obs[cols_to_keep_existing],\n",
    "        on=[\"game_id\", \"play_id\", \"nfl_id\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    if \"player_to_predict\" in train_raw.columns:\n",
    "        before = len(train_raw)\n",
    "        train_raw = train_raw[train_raw[\"player_to_predict\"].astype(bool)].copy()\n",
    "        after = len(train_raw)\n",
    "        print(f\" Filtered to player_to_predict==True: {before} -> {after} rows\")\n",
    "\n",
    "    train_main = create_features(train_raw, is_train=True)\n",
    "\n",
    "    # Symmetric augmentation along the Y-axis\n",
    "    train_mirror_raw = mirror_raw(train_raw)\n",
    "    train_mirror = create_features(train_mirror_raw, is_train=True)\n",
    "\n",
    "    train_tree = pd.concat([train_main, train_mirror], ignore_index=True)\n",
    "    print(f\" After symmetry augmentation (tree): {len(train_main)} -> {len(train_tree)} rows\")\n",
    "\n",
    "    # For GNN\n",
    "    train_gnn = train_main.copy()\n",
    "\n",
    "    return train_tree, train_gnn\n",
    "\n",
    "\n",
    "# ======================== GNN DATASET ============================\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, num_cols, cat_code_cols):\n",
    "        self.num_cols = list(num_cols)\n",
    "        self.cat_code_cols = list(cat_code_cols)\n",
    "\n",
    "        self.X_num = df[self.num_cols].to_numpy(np.float32)\n",
    "        if self.cat_code_cols:\n",
    "            self.X_cat = df[self.cat_code_cols].to_numpy(np.int64)\n",
    "        else:\n",
    "            self.X_cat = None\n",
    "        self.y = df[[\"dx\", \"dy\"]].to_numpy(np.float32)\n",
    "        self.w = (1.0 + TARGET_WEIGHT_GNN * df[\"is_target\"].to_numpy(np.float32)).astype(\n",
    "            np.float32\n",
    "        )\n",
    "\n",
    "        if \"frame_id\" in df.columns:\n",
    "            gkeys = (\n",
    "                df[\"game_id\"].astype(str)\n",
    "                + \"_\"\n",
    "                + df[\"play_id\"].astype(str)\n",
    "                + \"_\"\n",
    "                + df[\"frame_id\"].astype(str)\n",
    "            )\n",
    "        else:\n",
    "            gkeys = (\n",
    "                df[\"game_id\"].astype(str)\n",
    "                + \"_\"\n",
    "                + df[\"play_id\"].astype(str)\n",
    "            )\n",
    "\n",
    "        self.graph_ids, _ = pd.factorize(gkeys)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.X_cat is not None:\n",
    "            return (\n",
    "                self.X_num[idx],\n",
    "                self.X_cat[idx],\n",
    "                self.y[idx],\n",
    "                self.w[idx],\n",
    "                int(self.graph_ids[idx]),\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                self.X_num[idx],\n",
    "                None,\n",
    "                self.y[idx],\n",
    "                self.w[idx],\n",
    "                int(self.graph_ids[idx]),\n",
    "            )\n",
    "\n",
    "\n",
    "def collate_graph(batch):\n",
    "    X_num, X_cat, y, w, g_ids = zip(*batch)\n",
    "    X_num = torch.from_numpy(np.stack(X_num))\n",
    "    if X_cat[0] is not None:\n",
    "        X_cat = torch.from_numpy(np.stack(X_cat))\n",
    "    else:\n",
    "        X_cat = None\n",
    "    y = torch.from_numpy(np.stack(y))\n",
    "    w = torch.from_numpy(np.stack(w))\n",
    "    g_ids = torch.tensor(g_ids, dtype=torch.long)\n",
    "    return X_num, X_cat, y, w, g_ids\n",
    "\n",
    "\n",
    "# ======================== GNN LAYERS =============================\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, alpha=0.2):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.W = nn.Linear(in_features, out_features, bias=False)\n",
    "        self.a = nn.Linear(2 * out_features, 1, bias=False)\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        Wh = self.W(h)  # [N, F_out]\n",
    "        N = Wh.size(0)\n",
    "\n",
    "        Wh_i = Wh.unsqueeze(1).repeat(1, N, 1)\n",
    "        Wh_j = Wh.unsqueeze(0).repeat(N, 1, 1)\n",
    "\n",
    "        e_input = torch.cat([Wh_i, Wh_j], dim=2)\n",
    "        e = self.leakyrelu(self.a(e_input).squeeze(2))\n",
    "\n",
    "        e = e.masked_fill(adj == 0, float(\"-inf\"))\n",
    "        attention = torch.softmax(e, dim=1)\n",
    "\n",
    "        h_prime = attention @ Wh\n",
    "        return h_prime\n",
    "\n",
    "\n",
    "class MultiHeadGAT(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_heads=2, alpha=0.2):\n",
    "        super().__init__()\n",
    "        assert out_features % num_heads == 0\n",
    "        head_dim = out_features // num_heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [GraphAttentionLayer(in_features, head_dim, alpha=alpha) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.out_proj = nn.Linear(out_features, out_features)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        head_outs = [head(h, adj) for head in self.heads]\n",
    "        h_cat = torch.cat(head_outs, dim=1)\n",
    "        out = self.out_proj(h_cat)\n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_features, out_features, bias=False)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        N = adj.size(0)\n",
    "        I = torch.eye(N, device=adj.device)\n",
    "        A_hat = adj + I\n",
    "        deg = A_hat.sum(dim=1)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[torch.isinf(deg_inv_sqrt)] = 0.0\n",
    "        D_inv_sqrt = torch.diag(deg_inv_sqrt)\n",
    "        A_norm = D_inv_sqrt @ A_hat @ D_inv_sqrt\n",
    "\n",
    "        hW = self.W(h)\n",
    "        out = A_norm @ hW\n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "class GATGCNModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_num_features: int,\n",
    "        num_cat_features: int,\n",
    "        cat_cardinalities,\n",
    "        hidden_dim: int = 384,\n",
    "        emb_dim: int = 16,\n",
    "        gat_heads: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_num_features = num_num_features\n",
    "        self.num_cat_features = num_cat_features\n",
    "\n",
    "        self.emb_layers = nn.ModuleList()\n",
    "        total_emb_dim = 0\n",
    "        if num_cat_features > 0:\n",
    "            for card in cat_cardinalities:\n",
    "                dim = min(emb_dim, (card + 1) // 2)\n",
    "                self.emb_layers.append(nn.Embedding(card, dim))\n",
    "                total_emb_dim += dim\n",
    "\n",
    "        self.num_linear = nn.Linear(num_num_features, hidden_dim)\n",
    "        self.cat_linear = nn.Linear(total_emb_dim, hidden_dim) if total_emb_dim > 0 else None\n",
    "\n",
    "        self.gat1 = MultiHeadGAT(hidden_dim, hidden_dim, num_heads=gat_heads)\n",
    "        self.gat2 = MultiHeadGAT(hidden_dim, hidden_dim, num_heads=gat_heads)\n",
    "        self.gcn1 = GCNLayer(hidden_dim, hidden_dim)\n",
    "        self.gcn2 = GCNLayer(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.out_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_num, x_cat, adj):\n",
    "        h_num = self.num_linear(x_num)\n",
    "\n",
    "        if self.emb_layers and x_cat is not None:\n",
    "            embs = []\n",
    "            for i, emb_layer in enumerate(self.emb_layers):\n",
    "                embs.append(emb_layer(x_cat[:, i]))\n",
    "            cat_emb = torch.cat(embs, dim=1)\n",
    "            h_cat = self.cat_linear(cat_emb) if self.cat_linear is not None else 0.0\n",
    "            h = F.relu(h_num + h_cat)\n",
    "        else:\n",
    "            h = F.relu(h_num)\n",
    "\n",
    "        h = self.gat1(h, adj)\n",
    "        h = self.dropout(h)\n",
    "        h = self.gat2(h, adj)\n",
    "        h = self.dropout(h)\n",
    "        h = self.gcn1(h, adj)\n",
    "        h = self.dropout(h)\n",
    "        h = self.gcn2(h, adj)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        out = self.out_mlp(h)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_gnn_model(train_gnn: pd.DataFrame):\n",
    "    global GNN_MODEL\n",
    "\n",
    "    print(\" Training GAT+GCN model (dx, dy)...\")\n",
    "\n",
    "    for col in GNN_NUM_FEATS:\n",
    "        if col not in train_gnn.columns:\n",
    "            train_gnn[col] = 0.0\n",
    "\n",
    "    for c in CAT_FEATS:\n",
    "        cats = CAT_CATEGORY_MAPS[c]\n",
    "        codes = pd.Categorical(train_gnn[c], categories=cats).codes\n",
    "        codes = np.where(codes < 0, len(cats), codes)\n",
    "        train_gnn[f\"{c}_cat\"] = codes.astype(\"int64\")\n",
    "\n",
    "    train_gnn_scaled = train_gnn.copy()\n",
    "    train_gnn_scaled[GNN_NUM_FEATS] = (\n",
    "        (train_gnn_scaled[GNN_NUM_FEATS] - GNN_NUM_MEAN) / GNN_NUM_STD\n",
    "    ).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    dataset = GraphDataset(train_gnn_scaled, GNN_NUM_FEATS, GNN_CAT_CODE_COLS)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=GNN_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_graph,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    cat_cardinalities = [CAT_CARD_SIZES[c] for c in CAT_FEATS]\n",
    "\n",
    "    model = GATGCNModel(\n",
    "        num_num_features=len(GNN_NUM_FEATS),\n",
    "        num_cat_features=len(GNN_CAT_CODE_COLS),\n",
    "        cat_cardinalities=cat_cardinalities,\n",
    "        hidden_dim=GNN_HIDDEN_DIM,\n",
    "        emb_dim=GNN_EMB_DIM,\n",
    "        gat_heads=GNN_GAT_HEADS,\n",
    "        dropout=GNN_DROPOUT,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=GNN_LR, weight_decay=1e-4)\n",
    "    loss_fn = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(GNN_EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        n_samples = 0\n",
    "        for X_num, X_cat, y, w, g_ids in loader:\n",
    "            X_num = X_num.to(DEVICE)\n",
    "            X_cat = X_cat.to(DEVICE) if X_cat is not None else None\n",
    "            y = y.to(DEVICE)\n",
    "            w = w.to(DEVICE)\n",
    "            g_ids = g_ids.to(DEVICE)\n",
    "\n",
    "            adj = (g_ids.unsqueeze(1) == g_ids.unsqueeze(0)).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X_num, X_cat, adj)\n",
    "            loss_per_coord = loss_fn(pred, y).mean(dim=1)\n",
    "            loss = (loss_per_coord * w).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * y.size(0)\n",
    "            n_samples += y.size(0)\n",
    "\n",
    "        avg_loss = running_loss / max(1, n_samples)\n",
    "        print(f\"  GNN epoch {epoch + 1}/{GNN_EPOCHS}: loss = {avg_loss:.6f}\")\n",
    "\n",
    "    GNN_MODEL = model\n",
    "    print(\"✓ GNN model trained\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# ======================== TUNING & SAVING HELPERS =================\n",
    "def rmse_xy(x_true, y_true, x_pred, y_pred) -> float:\n",
    "    \"\"\"Euclidean RMSE in (x, y).\"\"\"\n",
    "    err = np.sqrt((x_pred - x_true) ** 2 + (y_pred - y_true) ** 2)\n",
    "    return float(err.mean())\n",
    "\n",
    "\n",
    "def make_holdout_split(df, val_frac=0.2, random_state=RANDOM_STATE):\n",
    "    \"\"\"Simple random holdout split for tuning.\"\"\"\n",
    "    val = df.sample(frac=val_frac, random_state=random_state)\n",
    "    train = df.drop(val.index)\n",
    "    return train.reset_index(drop=True), val.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def tune_lgbm_hyperparams(train_tree: pd.DataFrame, max_train_rows: int = 300_000):\n",
    "    \"\"\"\n",
    "    Light but more systematic LGBM tuning on a random holdout of train_tree.\n",
    "    - Uses the same random-holdout strategy (so comparable to your earlier 0.70 / 0.60 baselines).\n",
    "    - Explores a slightly richer grid of (num_leaves, learning_rate, n_estimators).\n",
    "    \"\"\"\n",
    "    print(\"\\n[2.5] Light hyperparameter tuning for LGBM...\")\n",
    "\n",
    "    # Subsample for speed\n",
    "    if len(train_tree) > max_train_rows:\n",
    "        work_df = train_tree.sample(n=max_train_rows, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "        print(f\"  Subsampled train_tree: {len(train_tree)} -> {len(work_df)} rows for tuning\")\n",
    "    else:\n",
    "        work_df = train_tree.reset_index(drop=True)\n",
    "\n",
    "    # Holdout split (same idea as before, so we can compare RMSEs)\n",
    "    train_df, val_df = make_holdout_split(work_df, val_frac=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Ensure feature / cat columns\n",
    "    for col in FEATURES:\n",
    "        if col not in train_df.columns:\n",
    "            train_df[col] = 0.0\n",
    "            val_df[col] = 0.0\n",
    "    for c in CAT_FEATS:\n",
    "        if c not in train_df.columns:\n",
    "            train_df[c] = \"unknown\"\n",
    "            val_df[c] = \"unknown\"\n",
    "        train_df[c] = train_df[c].astype(\"category\")\n",
    "        val_df[c] = val_df[c].astype(\"category\")\n",
    "\n",
    "    X_tr = train_df[FEATURES + CAT_FEATS].copy()\n",
    "    X_va = val_df[FEATURES + CAT_FEATS].copy()\n",
    "\n",
    "    y_tr_dx = train_df[\"dx\"].values\n",
    "    y_tr_dy = train_df[\"dy\"].values\n",
    "    y_va_dx = val_df[\"dx\"].values\n",
    "    y_va_dy = val_df[\"dy\"].values\n",
    "\n",
    "    w_tr = (1.0 + TARGET_WEIGHT_TREE * train_df[\"is_target\"].values.astype(np.float32)).astype(np.float32)\n",
    "\n",
    "    base_params = dict(\n",
    "        objective=\"regression\",\n",
    "        boosting_type=\"gbdt\",\n",
    "        min_data_in_leaf=50,\n",
    "        feature_fraction=0.9,\n",
    "        bagging_fraction=0.9,\n",
    "        bagging_freq=1,\n",
    "        verbosity=-1,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    # Includes “base” parameters and some nearby configs\n",
    "    search_space = [\n",
    "        # Original-ish baseline-ish config\n",
    "        {\"num_leaves\": 63,  \"learning_rate\": 0.05, \"n_estimators\": 800},\n",
    "        # Your previous best\n",
    "        {\"num_leaves\": 127, \"learning_rate\": 0.05, \"n_estimators\": 1000},\n",
    "        # Slightly faster LR, fewer trees\n",
    "        {\"num_leaves\": 127, \"learning_rate\": 0.07, \"n_estimators\": 800},\n",
    "        # Deeper trees, more capacity\n",
    "        {\"num_leaves\": 255, \"learning_rate\": 0.05, \"n_estimators\": 1200},\n",
    "        # Deeper, smaller LR (more boosting steps)\n",
    "        {\"num_leaves\": 255, \"learning_rate\": 0.03, \"n_estimators\": 1600},\n",
    "        # Smaller leaves, smaller LR\n",
    "        {\"num_leaves\": 63,  \"learning_rate\": 0.03, \"n_estimators\": 1400},\n",
    "    ]\n",
    "\n",
    "    best_rmse = float(\"inf\")\n",
    "    best_cfg = None\n",
    "\n",
    "    x_last_va = val_df[\"x_last\"].values\n",
    "    y_last_va = val_df[\"y_last\"].values\n",
    "    x_true_va = val_df[\"x\"].values\n",
    "    y_true_va = val_df[\"y\"].values\n",
    "\n",
    "    for cfg in search_space:\n",
    "        params = dict(base_params)\n",
    "        params.update(cfg)\n",
    "        print(f\"  Trying LGBM params: {cfg}\")\n",
    "\n",
    "        mdl_dx = LGBMRegressor(**params)\n",
    "        mdl_dx.fit(X_tr, y_tr_dx, categorical_feature=CAT_FEATS, sample_weight=w_tr)\n",
    "\n",
    "        mdl_dy = LGBMRegressor(**params)\n",
    "        mdl_dy.fit(X_tr, y_tr_dy, categorical_feature=CAT_FEATS, sample_weight=w_tr)\n",
    "\n",
    "        pred_dx = mdl_dx.predict(X_va)\n",
    "        pred_dy = mdl_dy.predict(X_va)\n",
    "\n",
    "        x_pred = x_last_va + pred_dx\n",
    "        y_pred = y_last_va + pred_dy\n",
    "        rmse_val = rmse_xy(x_true_va, y_true_va, x_pred, y_pred)\n",
    "        print(f\"    -> val RMSE: {rmse_val:.4f}\")\n",
    "\n",
    "        if rmse_val < best_rmse:\n",
    "            best_rmse = rmse_val\n",
    "            best_cfg = cfg\n",
    "\n",
    "    print(f\"  Best LGBM params: {best_cfg}, val RMSE: {best_rmse:.4f}\")\n",
    "    return best_cfg\n",
    "\n",
    "\n",
    "\n",
    "def tune_xgb_hyperparams(train_tree: pd.DataFrame, max_train_rows: int = 300_000):\n",
    "    \"\"\"\n",
    "    Light but more systematic XGBoost tuning on a random holdout of train_tree.\n",
    "    Uses the same holdout logic for comparability.\n",
    "    \"\"\"\n",
    "    print(\"\\n[2.6] Light hyperparameter tuning for XGBoost...\")\n",
    "\n",
    "    if len(train_tree) > max_train_rows:\n",
    "        work_df = train_tree.sample(n=max_train_rows, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "        print(f\"  Subsampled train_tree: {len(train_tree)} -> {len(work_df)} rows for tuning\")\n",
    "    else:\n",
    "        work_df = train_tree.reset_index(drop=True)\n",
    "\n",
    "    train_df, val_df = make_holdout_split(work_df, val_frac=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "    for col in FEATURES:\n",
    "        if col not in train_df.columns:\n",
    "            train_df[col] = 0.0\n",
    "            val_df[col] = 0.0\n",
    "    for c in CAT_FEATS:\n",
    "        if c not in train_df.columns:\n",
    "            train_df[c] = \"unknown\"\n",
    "            val_df[c] = \"unknown\"\n",
    "        train_df[c] = train_df[c].astype(\"category\")\n",
    "        val_df[c] = val_df[c].astype(\"category\")\n",
    "\n",
    "    num_cols = FEATURES\n",
    "\n",
    "    X_tr = train_df[FEATURES + CAT_FEATS].copy()\n",
    "    X_va = val_df[FEATURES + CAT_FEATS].copy()\n",
    "\n",
    "    # numeric cleaning\n",
    "    X_tr[num_cols] = X_tr[num_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    X_va[num_cols] = X_va[num_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    for c in CAT_FEATS:\n",
    "        X_tr[c] = X_tr[c].astype(\"category\")\n",
    "        X_va[c] = X_va[c].astype(\"category\")\n",
    "\n",
    "    y_tr_dx = train_df[\"dx\"].values\n",
    "    y_tr_dy = train_df[\"dy\"].values\n",
    "    y_va_dx = val_df[\"dx\"].values\n",
    "    y_va_dy = val_df[\"dy\"].values\n",
    "\n",
    "    w_tr = (1.0 + TARGET_WEIGHT_TREE * train_df[\"is_target\"].values.astype(np.float32)).astype(np.float32)\n",
    "\n",
    "    base_params = dict(\n",
    "        objective=\"reg:squarederror\",\n",
    "        tree_method=\"hist\",\n",
    "        enable_categorical=True,\n",
    "        n_jobs=-1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    # A slightly richer but still small grid\n",
    "    search_space = [\n",
    "        {\"max_depth\": 6,  \"learning_rate\": 0.05, \"n_estimators\": 400, \"subsample\": 0.9, \"colsample_bytree\": 0.9},\n",
    "        {\"max_depth\": 8,  \"learning_rate\": 0.05, \"n_estimators\": 600, \"subsample\": 0.9, \"colsample_bytree\": 0.9},\n",
    "        {\"max_depth\": 8,  \"learning_rate\": 0.07, \"n_estimators\": 500, \"subsample\": 0.9, \"colsample_bytree\": 0.9},\n",
    "        {\"max_depth\": 10, \"learning_rate\": 0.05, \"n_estimators\": 700, \"subsample\": 0.85, \"colsample_bytree\": 0.8},\n",
    "        {\"max_depth\": 6,  \"learning_rate\": 0.03, \"n_estimators\": 800, \"subsample\": 0.9, \"colsample_bytree\": 0.9},\n",
    "    ]\n",
    "\n",
    "    best_rmse = float(\"inf\")\n",
    "    best_cfg = None\n",
    "\n",
    "    x_last_va = val_df[\"x_last\"].values\n",
    "    y_last_va = val_df[\"y_last\"].values\n",
    "    x_true_va = val_df[\"x\"].values\n",
    "    y_true_va = val_df[\"y\"].values\n",
    "\n",
    "    for cfg in search_space:\n",
    "        params = dict(base_params)\n",
    "        params.update(cfg)\n",
    "        print(f\"  Trying XGB params: {cfg}\")\n",
    "\n",
    "        mdl_dx = xgb.XGBRegressor(**params)\n",
    "        mdl_dx.fit(X_tr, y_tr_dx, sample_weight=w_tr)\n",
    "\n",
    "        mdl_dy = xgb.XGBRegressor(**params)\n",
    "        mdl_dy.fit(X_tr, y_tr_dy, sample_weight=w_tr)\n",
    "\n",
    "        pred_dx = mdl_dx.predict(X_va)\n",
    "        pred_dy = mdl_dy.predict(X_va)\n",
    "\n",
    "        x_pred = x_last_va + pred_dx\n",
    "        y_pred = y_last_va + pred_dy\n",
    "        rmse_val = rmse_xy(x_true_va, y_true_va, x_pred, y_pred)\n",
    "        print(f\"    -> val RMSE: {rmse_val:.4f}\")\n",
    "\n",
    "        if rmse_val < best_rmse:\n",
    "            best_rmse = rmse_val\n",
    "            best_cfg = cfg\n",
    "\n",
    "    print(f\"  Best XGB params: {best_cfg}, val RMSE: {best_rmse:.4f}\")\n",
    "    return best_cfg\n",
    "\n",
    "\n",
    "def tune_ensemble_weights(train_tree: pd.DataFrame, max_rows: int = 50_000):\n",
    "    \"\"\"\n",
    "    Systematic search for best (w_lgbm, w_xgb, w_gnn) on a subset of train_tree.\n",
    "    - Computes individual model RMSE first.\n",
    "    - Uses a grid on the simplex of weights instead of a few arbitrary guesses.\n",
    "    \"\"\"\n",
    "    global ENSEMBLE_WEIGHTS\n",
    "\n",
    "    print(\"\\n[4/4] Tuning ensemble weights (LGBM / XGB / GNN)...\")\n",
    "\n",
    "    # Subsample for speed\n",
    "    if len(train_tree) > max_rows:\n",
    "        work_df = train_tree.sample(n=max_rows, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "        print(f\"  Subsampled train_tree: {len(train_tree)} -> {len(work_df)} rows for ensemble tuning\")\n",
    "    else:\n",
    "        work_df = train_tree.reset_index(drop=True)\n",
    "\n",
    "    # Ensure features + cats\n",
    "    for col in FEATURES:\n",
    "        if col not in work_df.columns:\n",
    "            work_df[col] = 0.0\n",
    "    for c in CAT_FEATS:\n",
    "        if c not in work_df.columns:\n",
    "            work_df[c] = \"unknown\"\n",
    "        work_df[c] = work_df[c].astype(\"category\")\n",
    "\n",
    "    # ----- LGBM predictions -----\n",
    "    X_tree = work_df[FEATURES + CAT_FEATS].copy()\n",
    "    for c in CAT_FEATS:\n",
    "        X_tree[c] = X_tree[c].astype(\"category\")\n",
    "\n",
    "    if LGBM_MODELS_DX:\n",
    "        pred_dx_lgb_list = [m.predict(X_tree) for m in LGBM_MODELS_DX]\n",
    "        pred_dy_lgb_list = [m.predict(X_tree) for m in LGBM_MODELS_DY]\n",
    "        pred_dx_lgb = np.mean(pred_dx_lgb_list, axis=0)\n",
    "        pred_dy_lgb = np.mean(pred_dy_lgb_list, axis=0)\n",
    "    else:\n",
    "        pred_dx_lgb = np.zeros(len(work_df), dtype=np.float32)\n",
    "        pred_dy_lgb = np.zeros(len(work_df), dtype=np.float32)\n",
    "\n",
    "    # ----- XGB predictions -----\n",
    "    num_cols = FEATURES\n",
    "    X_xgb = work_df[FEATURES + CAT_FEATS].copy()\n",
    "    X_xgb[num_cols] = X_xgb[num_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    for c in CAT_FEATS:\n",
    "        X_xgb[c] = X_xgb[c].astype(\"category\")\n",
    "\n",
    "    if XGB_MODELS_DX:\n",
    "        pred_dx_xgb_list = [m.predict(X_xgb) for m in XGB_MODELS_DX]\n",
    "        pred_dy_xgb_list = [m.predict(X_xgb) for m in XGB_MODELS_DY]\n",
    "        pred_dx_xgb = np.mean(pred_dx_xgb_list, axis=0)\n",
    "        pred_dy_xgb = np.mean(pred_dy_xgb_list, axis=0)\n",
    "    else:\n",
    "        pred_dx_xgb = np.zeros(len(work_df), dtype=np.float32)\n",
    "        pred_dy_xgb = np.zeros(len(work_df), dtype=np.float32)\n",
    "\n",
    "    # ----- GNN predictions -----\n",
    "    if GNN_MODEL is not None and GNN_NUM_MEAN is not None and GNN_NUM_STD is not None:\n",
    "        df_g = work_df.copy()\n",
    "        for c in CAT_FEATS:\n",
    "            cats = CAT_CATEGORY_MAPS[c]\n",
    "            codes = pd.Categorical(df_g[c], categories=cats).codes\n",
    "            codes = np.where(codes < 0, len(cats), codes)\n",
    "            df_g[f\"{c}_cat\"] = codes.astype(\"int64\")\n",
    "\n",
    "        for col in GNN_NUM_FEATS:\n",
    "            if col not in df_g.columns:\n",
    "                df_g[col] = 0.0\n",
    "\n",
    "        X_num_df = df_g[GNN_NUM_FEATS].copy()\n",
    "        X_num_df = (\n",
    "            (X_num_df - GNN_NUM_MEAN) / GNN_NUM_STD\n",
    "        ).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "        X_num = X_num_df.to_numpy(np.float32)\n",
    "        X_cat = df_g[GNN_CAT_CODE_COLS].to_numpy(np.int64)\n",
    "\n",
    "        if \"frame_id\" in df_g.columns:\n",
    "            gkeys = (\n",
    "                df_g[\"game_id\"].astype(str)\n",
    "                + \"_\"\n",
    "                + df_g[\"play_id\"].astype(str)\n",
    "                + \"_\"\n",
    "                + df_g[\"frame_id\"].astype(str)\n",
    "            )\n",
    "        else:\n",
    "            gkeys = (\n",
    "                df_g[\"game_id\"].astype(str)\n",
    "                + \"_\"\n",
    "                + df_g[\"play_id\"].astype(str)\n",
    "            )\n",
    "        g_ids, _ = pd.factorize(gkeys)\n",
    "\n",
    "        unique_graphs = np.unique(g_ids)\n",
    "        pred_dx_gnn = np.zeros(len(df_g), dtype=np.float32)\n",
    "        pred_dy_gnn = np.zeros(len(df_g), dtype=np.float32)\n",
    "\n",
    "        GNN_MODEL.eval()\n",
    "        with torch.no_grad():\n",
    "            for graph_id in unique_graphs:\n",
    "                mask = g_ids == graph_id\n",
    "                if mask.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                X_num_batch = torch.from_numpy(X_num[mask]).to(DEVICE)\n",
    "                X_cat_batch = torch.from_numpy(X_cat[mask]).to(DEVICE)\n",
    "\n",
    "                n_nodes = X_num_batch.size(0)\n",
    "                adj_batch = torch.ones(n_nodes, n_nodes, device=DEVICE)  # fully connected\n",
    "\n",
    "                pred_nn_batch = GNN_MODEL(X_num_batch, X_cat_batch, adj_batch).cpu().numpy()\n",
    "\n",
    "                pred_dx_gnn[mask] = pred_nn_batch[:, 0]\n",
    "                pred_dy_gnn[mask] = pred_nn_batch[:, 1]\n",
    "\n",
    "        print(f\"  Processed {len(unique_graphs)} graphs for GNN predictions\")\n",
    "    else:\n",
    "        pred_dx_gnn = np.zeros(len(work_df), dtype=np.float32)\n",
    "        pred_dy_gnn = np.zeros(len(work_df), dtype=np.float32)\n",
    "\n",
    "    x_last = work_df[\"x_last\"].values\n",
    "    y_last = work_df[\"y_last\"].values\n",
    "    x_true = work_df[\"x\"].values\n",
    "    y_true = work_df[\"y\"].values\n",
    "\n",
    "    # ---- First: individual model RMSEs ----\n",
    "    def rmse_for(dx, dy, label):\n",
    "        x_pred = x_last + dx\n",
    "        y_pred = y_last + dy\n",
    "        rm = rmse_xy(x_true, y_true, x_pred, y_pred)\n",
    "        print(f\"  Single-model RMSE [{label}] = {rm:.4f}\")\n",
    "        return rm\n",
    "\n",
    "    has_lgb = LGBM_MODELS_DX is not None and len(LGBM_MODELS_DX) > 0\n",
    "    has_xgb = XGB_MODELS_DX is not None and len(XGB_MODELS_DX) > 0\n",
    "    has_gnn = GNN_MODEL is not None\n",
    "\n",
    "    rm_lgb = rm_xgb = rm_gnn = None\n",
    "\n",
    "    if has_lgb:\n",
    "        rm_lgb = rmse_for(pred_dx_lgb, pred_dy_lgb, \"LGBM\")\n",
    "    if has_xgb:\n",
    "        rm_xgb = rmse_for(pred_dx_xgb, pred_dy_xgb, \"XGB\")\n",
    "    if has_gnn:\n",
    "        rm_gnn = rmse_for(pred_dx_gnn, pred_dy_gnn, \"GNN\")\n",
    "\n",
    "    # ---- Now: systematic grid search over weights ----\n",
    "    best_rmse = float(\"inf\")\n",
    "    best_w = (1.0, 0.0, 0.0)\n",
    "\n",
    "    if has_lgb and has_xgb and not has_gnn:\n",
    "        # 2-model simplex: w_l in [0,1] with step 0.05, w_x = 1 - w_l\n",
    "        print(\"  Searching weights for 2-model ensemble (LGBM+XGB) with step=0.05 ...\")\n",
    "        for w_l in np.linspace(0.0, 1.0, 21):\n",
    "            w_x = 1.0 - w_l\n",
    "            dx = w_l * pred_dx_lgb + w_x * pred_dx_xgb\n",
    "            dy = w_l * pred_dy_lgb + w_x * pred_dy_xgb\n",
    "            x_pred = x_last + dx\n",
    "            y_pred = y_last + dy\n",
    "            rm = rmse_xy(x_true, y_true, x_pred, y_pred)\n",
    "            if rm < best_rmse:\n",
    "                best_rmse = rm\n",
    "                best_w = (w_l, w_x, 0.0)\n",
    "\n",
    "    elif has_lgb and has_xgb and has_gnn:\n",
    "        # 3-model simplex: weights in {0.0, 0.1, ..., 1.0} with w_l + w_x + w_g = 1\n",
    "        print(\"  Searching weights for 3-model ensemble (LGBM+XGB+GNN) with step=0.1 ...\")\n",
    "        step = 0.1\n",
    "        grid_vals = np.arange(0.0, 1.0 + 1e-9, step)\n",
    "        for w_l in grid_vals:\n",
    "            for w_x in grid_vals:\n",
    "                w_g = 1.0 - w_l - w_x\n",
    "                if w_g < -1e-9 or w_g > 1.0:\n",
    "                    continue\n",
    "                # if GNN is very bad, don't waste a lot of time experimenting\n",
    "                if rm_gnn is not None and rm_lgb is not None and rm_gnn > rm_lgb + 0.5 and w_g > 0.3:\n",
    "                    continue\n",
    "\n",
    "                dx = w_l * pred_dx_lgb + w_x * pred_dx_xgb + w_g * pred_dx_gnn\n",
    "                dy = w_l * pred_dy_lgb + w_x * pred_dy_xgb + w_g * pred_dy_gnn\n",
    "                x_pred = x_last + dx\n",
    "                y_pred = y_last + dy\n",
    "                rm = rmse_xy(x_true, y_true, x_pred, y_pred)\n",
    "                if rm < best_rmse:\n",
    "                    best_rmse = rm\n",
    "                    best_w = (w_l, w_x, w_g)\n",
    "\n",
    "    elif has_lgb and not has_xgb and has_gnn:\n",
    "        # LGBM+GNN only\n",
    "        print(\"  Searching weights for 2-model ensemble (LGBM+GNN) with step=0.05 ...\")\n",
    "        for w_l in np.linspace(0.0, 1.0, 21):\n",
    "            w_g = 1.0 - w_l\n",
    "            dx = w_l * pred_dx_lgb + w_g * pred_dx_gnn\n",
    "            dy = w_l * pred_dy_lgb + w_g * pred_dy_gnn\n",
    "            x_pred = x_last + dx\n",
    "            y_pred = y_last + dy\n",
    "            rm = rmse_xy(x_true, y_true, x_pred, y_pred)\n",
    "            if rm < best_rmse:\n",
    "                best_rmse = rm\n",
    "                best_w = (w_l, 0.0, w_g)\n",
    "\n",
    "    else:\n",
    "        # Fallback: just use LGBM if available, otherwise XGB, otherwise GNN\n",
    "        print(\"  Not enough models for ensemble grid search; using best single model.\")\n",
    "        if rm_lgb is not None and (rm_xgb is None or rm_lgb <= rm_xgb) and (rm_gnn is None or rm_lgb <= rm_gnn):\n",
    "            best_rmse = rm_lgb\n",
    "            best_w = (1.0, 0.0, 0.0)\n",
    "        elif rm_xgb is not None and (rm_gnn is None or rm_xgb <= rm_gnn):\n",
    "            best_rmse = rm_xgb\n",
    "            best_w = (0.0, 1.0, 0.0)\n",
    "        elif rm_gnn is not None:\n",
    "            best_rmse = rm_gnn\n",
    "            best_w = (0.0, 0.0, 1.0)\n",
    "\n",
    "    ENSEMBLE_WEIGHTS = {\"lgbm\": best_w[0], \"xgb\": best_w[1], \"gnn\": best_w[2]}\n",
    "    print(\n",
    "        f\"  Best ensemble weights: LGBM={best_w[0]:.2f}, \"\n",
    "        f\"XGB={best_w[1]:.2f}, GNN={best_w[2]:.2f}, RMSE={best_rmse:.4f}\"\n",
    "    )\n",
    "    return ENSEMBLE_WEIGHTS\n",
    "\n",
    "\n",
    "\n",
    "def save_trained_models(output_dir: str = \"models\"):\n",
    "    \"\"\"\n",
    "    Save:\n",
    "      - all LGBM dx/dy models,\n",
    "      - all XGB dx/dy models,\n",
    "      - GNN model state_dict (if any),\n",
    "      - metadata (features, cats, category maps, GNN stats, ENSEMBLE_WEIGHTS).\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    out_dir = Path(output_dir)\n",
    "\n",
    "    for i, m in enumerate(LGBM_MODELS_DX):\n",
    "        joblib.dump(m, out_dir / f\"lgbm_dx_{i}.pkl\")\n",
    "    for i, m in enumerate(LGBM_MODELS_DY):\n",
    "        joblib.dump(m, out_dir / f\"lgbm_dy_{i}.pkl\")\n",
    "    print(f\"  Saved {len(LGBM_MODELS_DX)} LGBM dx and {len(LGBM_MODELS_DY)} LGBM dy models to {out_dir}\")\n",
    "\n",
    "    for i, m in enumerate(XGB_MODELS_DX):\n",
    "        joblib.dump(m, out_dir / f\"xgb_dx_{i}.pkl\")\n",
    "    for i, m in enumerate(XGB_MODELS_DY):\n",
    "        joblib.dump(m, out_dir / f\"xgb_dy_{i}.pkl\")\n",
    "    print(f\"  Saved {len(XGB_MODELS_DX)} XGB dx and {len(XGB_MODELS_DY)} XGB dy models to {out_dir}\")\n",
    "\n",
    "    if GNN_MODEL is not None:\n",
    "        torch.save(GNN_MODEL.state_dict(), out_dir / \"gnn_model.pth\")\n",
    "        print(\"  Saved GNN model state_dict to gnn_model.pth\")\n",
    "\n",
    "    meta = dict(\n",
    "        FEATURES=FEATURES,\n",
    "        CAT_FEATS=CAT_FEATS,\n",
    "        GNN_NUM_FEATS=GNN_NUM_FEATS,\n",
    "        CAT_CATEGORY_MAPS=CAT_CATEGORY_MAPS,\n",
    "        CAT_CARD_SIZES=CAT_CARD_SIZES,\n",
    "        GNN_NUM_MEAN=GNN_NUM_MEAN.to_dict() if GNN_NUM_MEAN is not None else None,\n",
    "        GNN_NUM_STD=GNN_NUM_STD.to_dict() if GNN_NUM_STD is not None else None,\n",
    "        ENSEMBLE_WEIGHTS=ENSEMBLE_WEIGHTS,\n",
    "        TARGET_WEIGHT_TREE=TARGET_WEIGHT_TREE,\n",
    "        TARGET_WEIGHT_GNN=TARGET_WEIGHT_GNN,\n",
    "    )\n",
    "    joblib.dump(meta, out_dir / \"meta.pkl\")\n",
    "    print(\"  Saved metadata to meta.pkl\")\n",
    "\n",
    "\n",
    "# ======================== MODEL TRAINING ==========================\n",
    "def train_models(train_tree: pd.DataFrame, train_gnn: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    1) Lightly tune LGBM hyperparams on a random holdout.\n",
    "    2) Lightly tune XGBoost hyperparams on a random holdout.\n",
    "    3) Train LGBM ensemble on full train_tree.\n",
    "    4) Train XGBoost models on full train_tree.\n",
    "    5) Train GNN on full train_gnn (if TRAIN_GNN=True).\n",
    "    6) Tune ensemble weights for LGBM/XGB/GNN on a subset of train_tree.\n",
    "    7) Save all models + metadata to disk.\n",
    "    \"\"\"\n",
    "    global CAT_CATEGORY_MAPS, CAT_CARD_SIZES\n",
    "    global GNN_NUM_MEAN, GNN_NUM_STD\n",
    "    global LGBM_MODELS_DX, LGBM_MODELS_DY\n",
    "    global XGB_MODELS_DX, XGB_MODELS_DY\n",
    "    global ENSEMBLE_WEIGHTS\n",
    "\n",
    "    print(\"\\n[3/4] Training models...\")\n",
    "\n",
    "    # Ensure features present\n",
    "    for col in FEATURES:\n",
    "        if col not in train_tree.columns:\n",
    "            train_tree[col] = 0.0\n",
    "        if col not in train_gnn.columns:\n",
    "            train_gnn[col] = 0.0\n",
    "    for col in CAT_FEATS:\n",
    "        if col not in train_tree.columns:\n",
    "            train_tree[col] = \"unknown\"\n",
    "        if col not in train_gnn.columns:\n",
    "            train_gnn[col] = \"unknown\"\n",
    "\n",
    "    # Categorical encodings for LGBM/XGB and dictionaries for GNN\n",
    "    for c in CAT_FEATS:\n",
    "        train_tree[c] = train_tree[c].astype(\"category\")\n",
    "        cats = list(train_tree[c].cat.categories)\n",
    "        CAT_CATEGORY_MAPS[c] = cats\n",
    "        CAT_CARD_SIZES[c] = len(cats) + 1  # +1 for unknown\n",
    "\n",
    "    # ---- 1) Light LGBM tuning ----\n",
    "    best_cfg_lgbm = tune_lgbm_hyperparams(train_tree)\n",
    "    if best_cfg_lgbm is None:\n",
    "        best_cfg_lgbm = {\"num_leaves\": 63, \"learning_rate\": 0.05, \"n_estimators\": 1200}\n",
    "        print(\"  WARNING: LGBM tuning did not return best_cfg; falling back to default:\", best_cfg_lgbm)\n",
    "\n",
    "    # ---- 2) Light XGB tuning ----\n",
    "    best_cfg_xgb = tune_xgb_hyperparams(train_tree)\n",
    "    if best_cfg_xgb is None:\n",
    "        best_cfg_xgb = {\n",
    "            \"max_depth\": 8,\n",
    "            \"learning_rate\": 0.05,\n",
    "            \"n_estimators\": 600,\n",
    "            \"subsample\": 0.9,\n",
    "            \"colsample_bytree\": 0.9,\n",
    "        }\n",
    "        print(\"  WARNING: XGB tuning did not return best_cfg; falling back to default:\", best_cfg_xgb)\n",
    "\n",
    "    # ---- 3) Train LGBM ensemble on FULL train_tree ----\n",
    "    X_tree = train_tree[FEATURES + CAT_FEATS].copy()\n",
    "    for c in CAT_FEATS:\n",
    "        X_tree[c] = X_tree[c].astype(\"category\")\n",
    "\n",
    "    y_dx = train_tree[\"dx\"].values\n",
    "    y_dy = train_tree[\"dy\"].values\n",
    "    sample_weight = (1.0 + TARGET_WEIGHT_TREE * train_tree[\"is_target\"].values).astype(np.float32)\n",
    "\n",
    "    base_params_lgbm = dict(\n",
    "        objective=\"regression\",\n",
    "        boosting_type=\"gbdt\",\n",
    "        n_estimators=best_cfg_lgbm[\"n_estimators\"],\n",
    "        learning_rate=best_cfg_lgbm[\"learning_rate\"],\n",
    "        num_leaves=best_cfg_lgbm[\"num_leaves\"],\n",
    "        min_data_in_leaf=50,\n",
    "        feature_fraction=0.9,\n",
    "        bagging_fraction=0.9,\n",
    "        bagging_freq=1,\n",
    "        verbosity=-1,\n",
    "    )\n",
    "\n",
    "    LGBM_MODELS_DX = []\n",
    "    LGBM_MODELS_DY = []\n",
    "\n",
    "    print(f\"\\n  Training LGBM ensemble with tuned params: {base_params_lgbm}\")\n",
    "    for m in range(LGBM_N_MODELS):\n",
    "        seed = RANDOM_STATE + m\n",
    "        params = dict(base_params_lgbm)\n",
    "        params[\"random_state\"] = seed\n",
    "\n",
    "        print(f\"   -> LGBM model {m + 1}/{LGBM_N_MODELS} for dx...\")\n",
    "        model_dx = LGBMRegressor(**params)\n",
    "        model_dx.fit(X_tree, y_dx, categorical_feature=CAT_FEATS, sample_weight=sample_weight)\n",
    "        LGBM_MODELS_DX.append(model_dx)\n",
    "\n",
    "        print(f\"   -> LGBM model {m + 1}/{LGBM_N_MODELS} for dy...\")\n",
    "        model_dy = LGBMRegressor(**params)\n",
    "        model_dy.fit(X_tree, y_dy, categorical_feature=CAT_FEATS, sample_weight=sample_weight)\n",
    "        LGBM_MODELS_DY.append(model_dy)\n",
    "\n",
    "    print(\"✓ LGBM ensemble trained on full dataset\")\n",
    "\n",
    "    # ---- 4) Train XGBoost models on FULL train_tree ----\n",
    "    num_cols = FEATURES\n",
    "    X_xgb_full = train_tree[FEATURES + CAT_FEATS].copy()\n",
    "    X_xgb_full[num_cols] = X_xgb_full[num_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    for c in CAT_FEATS:\n",
    "        X_xgb_full[c] = X_xgb_full[c].astype(\"category\")\n",
    "\n",
    "    base_params_xgb = dict(\n",
    "        objective=\"reg:squarederror\",\n",
    "        tree_method=\"hist\",\n",
    "        enable_categorical=True,\n",
    "        n_jobs=-1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "    base_params_xgb.update(best_cfg_xgb)\n",
    "\n",
    "    print(f\"\\n  Training XGBoost models with tuned params: {base_params_xgb}\")\n",
    "    XGB_MODELS_DX = []\n",
    "    XGB_MODELS_DY = []\n",
    "\n",
    "    # one model per target\n",
    "    print(\"   -> XGB model for dx...\")\n",
    "    xgb_dx = xgb.XGBRegressor(**base_params_xgb)\n",
    "    xgb_dx.fit(X_xgb_full, y_dx, sample_weight=sample_weight)\n",
    "    XGB_MODELS_DX.append(xgb_dx)\n",
    "\n",
    "    print(\"   -> XGB model for dy...\")\n",
    "    xgb_dy = xgb.XGBRegressor(**base_params_xgb)\n",
    "    xgb_dy.fit(X_xgb_full, y_dy, sample_weight=sample_weight)\n",
    "    XGB_MODELS_DY.append(xgb_dy)\n",
    "\n",
    "    print(\"✓ XGBoost models trained on full dataset\")\n",
    "\n",
    "    # ---- 5) GNN normalization stats + training ----\n",
    "    GNN_NUM_MEAN = train_gnn[GNN_NUM_FEATS].mean()\n",
    "    GNN_NUM_STD = train_gnn[GNN_NUM_FEATS].std().replace(0, 1.0).fillna(1.0)\n",
    "\n",
    "    if TRAIN_GNN:\n",
    "        train_gnn_model(train_gnn)\n",
    "    else:\n",
    "        print(\"  TRAIN_GNN=False -> skipping GNN training\")\n",
    "\n",
    "    # ---- 6) Tune ENSEMBLE_WEIGHTS ----\n",
    "    tune_ensemble_weights(train_tree)\n",
    "\n",
    "    # ---- 7) Save all models + metadata ----\n",
    "    save_trained_models(output_dir=\"models\")\n",
    "\n",
    "\n",
    "# ======================== INFERENCE HELPERS ======================\n",
    "def prepare_inference_batch(test_pd: pd.DataFrame, test_input_pd: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare rows for inference:\n",
    "    - take the last observation from test_input by (game_id, play_id, nfl_id),\n",
    "    - add the target receiver and pairwise features,\n",
    "    - merge with the current batch table test (id, game_id, play_id, nfl_id, frame_id),\n",
    "    - build features as in train.\n",
    "    \"\"\"\n",
    "    last_obs = prepare_last_obs(test_input_pd)\n",
    "    last_obs = add_target_info(last_obs)\n",
    "\n",
    "    cols_to_keep_existing = [c for c in BASE_COLS if c in last_obs.columns]\n",
    "\n",
    "    test_rows = test_pd.merge(\n",
    "        last_obs[cols_to_keep_existing],\n",
    "        on=[\"game_id\", \"play_id\", \"nfl_id\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    test_rows = create_features(test_rows, is_train=False)\n",
    "    return test_rows\n",
    "\n",
    "\n",
    "# ======================== PREDICT API (EVAL) =====================\n",
    "\n",
    "# ======================== MAIN (TRAIN + SERVER) ==================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Train on the public train dataset\n",
    "    df_in, df_out = load_train(DATA_DIR)\n",
    "    train_tree_df, train_gnn_df = prepare_train(df_in, df_out)\n",
    "    train_models(train_tree_df, train_gnn_df)\n",
    "\n",
    "    # 2) Start the inference server only if the module exists\n",
    "    if HAS_EVAL_SERVER and NFLInferenceServer is not None:\n",
    "        inference_server = NFLInferenceServer(predict)\n",
    "\n",
    "        if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "            inference_server.serve()\n",
    "        else:\n",
    "            print(\"\\n[LOCAL] Running local gateway to generate submission.csv on public mock test...\")\n",
    "            inference_server.run_local_gateway((DATA_DIR,))\n",
    "            print(\"✓ submission.csv should now be created in the working directory\")\n",
    "    else:\n",
    "        print(\n",
    "            \"\\nNFLInferenceServer unavailable (kaggle_evaluation not found). \"\n",
    "            \"Locally you can train the model and debug features, \"\n",
    "            \"but to generate submission.csv you need to run the code in Kaggle \"\n",
    "            \"with the dataset 'nfl-big-data-bowl-2026-prediction' connected in the Data tab.\"\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14210809,
     "sourceId": 114239,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

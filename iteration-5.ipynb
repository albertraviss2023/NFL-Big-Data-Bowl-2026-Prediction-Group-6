{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":114239,"databundleVersionId":14210809,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nNFL Big Data Bowl 2026\nTraining pipeline v2: LGBM + XGB + (optional) GNN\n\n- Loads public train data\n- Builds rich feature set (same as previous version)\n- Uses Optuna Bayesian optimization for LGBM and XGB hyperparameters\n- Trains:\n    * LGBM ensemble (3 models for dx, 3 for dy)\n    * XGB (1 dx, 1 dy)\n    * Optional GNN (same as before)\n- Tunes ensemble weights LGBM/XGB/GNN on holdout subset\n- Saves models + meta to ./models\n\nYou’ll then zip ./models and upload as a Kaggle dataset,\nand use the separate submission notebook for live inference.\n\"\"\"\n\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport joblib\n\nfrom lightgbm import LGBMRegressor\nimport xgboost as xgb\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# ----------------- Optional: Optuna for Bayesian optimization -----------------\ntry:\n    import optuna\n    HAS_OPTUNA = True\nexcept ModuleNotFoundError:\n    HAS_OPTUNA = False\n    print(\"WARNING: optuna not found. Will fall back to simple random search.\")\n\n# ======================================================================\n# CONFIG\n# ======================================================================\n\nDATA_DIR = \"/kaggle/input/nfl-big-data-bowl-2026-prediction\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nRANDOM_STATE = 42\n\n# How many trials for Optuna (tune carefully vs runtime constraints)\nN_TRIALS_LGBM = 30\nN_TRIALS_XGB = 25\n\n# LGBM ensemble size\nLGBM_N_MODELS = 3\n\n# GNN settings\nTRAIN_GNN = False      # set False if you want to skip GNN for speed\nGNN_EPOCHS = 5\nGNN_BATCH_SIZE = 512\nGNN_HIDDEN_DIM = 256\nGNN_EMB_DIM = 8\nGNN_GAT_HEADS = 2\nGNN_DROPOUT = 0.1\n\nTARGET_WEIGHT_TREE = 1.0\nTARGET_WEIGHT_GNN = 1.0\n\nnp.random.seed(RANDOM_STATE)\ntorch.manual_seed(RANDOM_STATE)\n\nprint(\"=\" * 70)\nprint(\"NFL BIG DATA BOWL 2026 - Training v2 (Bayesian LGBM + XGB + GNN)\")\nprint(\"=\" * 70)\nprint(\"DEVICE:\", DEVICE)\n\n# ======================================================================\n# FEATURES & GLOBAL STATE\n# ======================================================================\n\nFEATURES = [\n    \"x_last\", \"y_last\",\n    \"s\", \"a\", \"o\", \"dir\",\n    \"vx\", \"vy\",\n    \"ax_comp\", \"ay_comp\",\n    \"dir_sin\", \"dir_cos\",\n    \"o_sin\", \"o_cos\",\n    \"frame_offset\", \"time_offset\",\n    \"num_frames_output\",\n    \"frac_of_flight\",\n    \"frames_left\",\n    \"time_to_land\",\n    \"remaining_flight_frac\",\n    \"dist_to_ball_land\",\n    \"angle_to_ball_land\",\n    \"dist_to_ball_land_per_frame\",\n    \"cos_dir_to_ball\",\n    \"cos_orient_to_ball\",\n    \"x_rel_ball\",\n    \"y_rel_ball\",\n    \"v_toward_ball\",\n    \"v_across_ball\",\n    \"x_std\",\n    \"ball_land_x_std\",\n    \"dx_to_land_std\",\n    \"dy_to_land\",\n    \"dist_to_sideline\",\n    \"dist_to_center\",\n    \"yardline_100\",\n    \"yardline_norm\",\n    \"dist_to_endzone\",\n    \"dist_to_target_last\",\n    \"dx_to_target_last\",\n    \"dy_to_target_last\",\n    \"angle_to_target\",\n    \"cos_dir_to_target\",\n    \"cos_orient_to_target\",\n    \"v_toward_target\",\n    \"v_across_target\",\n    \"is_target\",\n    \"absolute_yardline_number\",\n    \"player_height\", \"player_weight\",\n    \"bmi\",\n    \"min_dist_teammate\",\n    \"mean_dist_teammate\",\n    \"min_dist_opponent\",\n    \"mean_dist_opponent\",\n]\n\nCAT_FEATS = [\"player_role\", \"player_side\", \"play_direction\"]\n\nBASE_COLS = [\n    \"game_id\", \"play_id\", \"nfl_id\",\n    \"x_last\", \"y_last\",\n    \"s\", \"a\", \"o\", \"dir\",\n    \"player_role\", \"player_side\",\n    \"num_frames_output\",\n    \"ball_land_x\", \"ball_land_y\",\n    \"target_last_x\", \"target_last_y\", \"target_nfl_id\",\n    \"play_direction\",\n    \"absolute_yardline_number\",\n    \"player_height\", \"player_weight\",\n    \"player_to_predict\",\n    \"min_dist_teammate\",\n    \"mean_dist_teammate\",\n    \"min_dist_opponent\",\n    \"mean_dist_opponent\",\n]\n\n# Global containers\nLGBM_MODELS_DX = []\nLGBM_MODELS_DY = []\nXGB_MODELS_DX = []\nXGB_MODELS_DY = []\n\nGNN_MODEL = None\n\nCAT_CATEGORY_MAPS: Dict[str, list] = {}\nCAT_CARD_SIZES: Dict[str, int] = {}\n\nGNN_NUM_FEATS = FEATURES\nGNN_CAT_CODE_COLS = [f\"{c}_cat\" for c in CAT_FEATS]\nGNN_NUM_MEAN = None\nGNN_NUM_STD = None\n\nENSEMBLE_WEIGHTS = {\"lgbm\": 1.0, \"xgb\": 0.0, \"gnn\": 0.0}\n\n# ======================================================================\n# BASIC UTILS\n# ======================================================================\n\ndef rmse_xy(x_true, y_true, x_pred, y_pred) -> float:\n    \"\"\"Euclidean RMSE in (x, y).\"\"\"\n    err = np.sqrt((x_pred - x_true) ** 2 + (y_pred - y_true) ** 2)\n    return float(err.mean())\n\n\ndef make_holdout_split(\n    df: pd.DataFrame,\n    val_frac: float = 0.2,\n    random_state: int = RANDOM_STATE,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Simple random holdout split for tuning.\"\"\"\n    val = df.sample(frac=val_frac, random_state=random_state)\n    train = df.drop(val.index)\n    return train.reset_index(drop=True), val.reset_index(drop=True)\n\n\n# ======================================================================\n# DATA LOADING\n# ======================================================================\n\ndef load_train(data_dir: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Load all train weeks input/output.\"\"\"\n    train_dir = os.path.join(data_dir, \"train\")\n    df_in_list = []\n    df_out_list = []\n\n    print(\"\\n[1/4] Loading training inputs/outputs...\")\n    for w in range(1, 19):\n        ip = os.path.join(train_dir, f\"input_2023_w{w:02d}.csv\")\n        op = os.path.join(train_dir, f\"output_2023_w{w:02d}.csv\")\n        if os.path.exists(ip) and os.path.exists(op):\n            df_i = pd.read_csv(ip)\n            df_o = pd.read_csv(op)\n            df_in_list.append(df_i)\n            df_out_list.append(df_o)\n            print(f\" Week {w:02d}: input {df_i.shape}, output {df_o.shape}\")\n        else:\n            print(f\" Week {w:02d}: files not found, skipping\")\n\n    if not df_in_list or not df_out_list:\n        raise FileNotFoundError(\n            f\"No train CSV files found in {train_dir}. \"\n            \"Check that the dataset is attached.\"\n        )\n\n    df_in = pd.concat(df_in_list, ignore_index=True)\n    df_out = pd.concat(df_out_list, ignore_index=True)\n    print(\"Train inputs:\", df_in.shape, \"train outputs:\", df_out.shape)\n    return df_in, df_out\n\n\n# ======================================================================\n# FEATURE ENGINEERING\n# ======================================================================\n\ndef height_to_inches(ht):\n    if isinstance(ht, str) and \"-\" in ht:\n        try:\n            feet, inches = ht.split(\"-\")\n            return int(feet) * 12 + int(inches)\n        except Exception:\n            return np.nan\n    return np.nan\n\n\ndef add_team_distance_features(df_last: pd.DataFrame) -> pd.DataFrame:\n    if \"player_side\" not in df_last.columns:\n        df_last[\"min_dist_teammate\"] = 0.0\n        df_last[\"mean_dist_teammate\"] = 0.0\n        df_last[\"min_dist_opponent\"] = 0.0\n        df_last[\"mean_dist_opponent\"] = 0.0\n        return df_last\n\n    groups = []\n    for (_, _), g in df_last.groupby([\"game_id\", \"play_id\"], as_index=False):\n        g = g.copy()\n        xs = g[\"x_last\"].to_numpy()\n        ys = g[\"y_last\"].to_numpy()\n        sides = g[\"player_side\"].astype(\"category\").cat.codes.to_numpy()\n\n        dx = xs[:, None] - xs[None, :]\n        dy = ys[:, None] - ys[None, :]\n        dist = np.sqrt(dx * dx + dy * dy)\n        np.fill_diagonal(dist, np.inf)\n\n        same = sides[:, None] == sides[None, :]\n        opp = ~same\n\n        dist_tm = np.where(same, dist, np.inf)\n        min_dist_tm = dist_tm.min(axis=1)\n        min_dist_tm[np.isinf(min_dist_tm)] = 0.0\n\n        sum_tm = np.where(same, dist, 0.0).sum(axis=1)\n        cnt_tm = same.sum(axis=1) - 1\n        mean_tm = np.divide(\n            sum_tm,\n            np.maximum(cnt_tm, 1),\n            out=np.zeros_like(sum_tm),\n            where=cnt_tm > 0,\n        )\n\n        dist_op = np.where(opp, dist, np.inf)\n        min_dist_op = dist_op.min(axis=1)\n        min_dist_op[np.isinf(min_dist_op)] = 0.0\n\n        sum_op = np.where(opp, dist, 0.0).sum(axis=1)\n        cnt_op = opp.sum(axis=1)\n        mean_op = np.divide(\n            sum_op,\n            np.maximum(cnt_op, 1),\n            out=np.zeros_like(sum_op),\n            where=cnt_op > 0,\n        )\n\n        g[\"min_dist_teammate\"] = min_dist_tm\n        g[\"mean_dist_teammate\"] = mean_tm\n        g[\"min_dist_opponent\"] = min_dist_op\n        g[\"mean_dist_opponent\"] = mean_op\n\n        groups.append(g)\n\n    return pd.concat(groups, ignore_index=True)\n\n\ndef prepare_last_obs(df: pd.DataFrame) -> pd.DataFrame:\n    df_last = (\n        df.sort_values([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n          .groupby([\"game_id\", \"play_id\", \"nfl_id\"], as_index=False)\n          .last()\n    )\n    df_last = df_last.rename(columns={\"x\": \"x_last\", \"y\": \"y_last\"})\n\n    if \"player_height\" in df_last.columns:\n        df_last[\"player_height\"] = df_last[\"player_height\"].apply(height_to_inches)\n    else:\n        df_last[\"player_height\"] = np.nan\n\n    df_last = add_team_distance_features(df_last)\n    return df_last\n\n\ndef add_target_info(df_last: pd.DataFrame) -> pd.DataFrame:\n    mask_target = df_last.get(\"player_role\", \"\") == \"Targeted Receiver\"\n    targets = df_last.loc[\n        mask_target,\n        [\"game_id\", \"play_id\", \"nfl_id\", \"x_last\", \"y_last\"],\n    ].copy()\n\n    targets = targets.rename(\n        columns={\n            \"nfl_id\": \"target_nfl_id\",\n            \"x_last\": \"target_last_x\",\n            \"y_last\": \"target_last_y\",\n        }\n    )\n\n    df_last = df_last.merge(\n        targets[[\"game_id\", \"play_id\", \"target_last_x\", \"target_last_y\", \"target_nfl_id\"]],\n        on=[\"game_id\", \"play_id\"],\n        how=\"left\",\n    )\n    return df_last\n\n\ndef mirror_raw(df_raw: pd.DataFrame) -> pd.DataFrame:\n    df = df_raw.copy()\n\n    if \"y_last\" in df.columns:\n        df[\"y_last\"] = 53.3 - df[\"y_last\"]\n    if \"y\" in df.columns:\n        df[\"y\"] = 53.3 - df[\"y\"]\n    if \"ball_land_y\" in df.columns:\n        df[\"ball_land_y\"] = 53.3 - df[\"ball_land_y\"]\n    if \"target_last_y\" in df.columns:\n        df[\"target_last_y\"] = 53.3 - df[\"target_last_y\"]\n\n    for ang_col in [\"dir\", \"o\"]:\n        if ang_col in df.columns:\n            df[ang_col] = (-df[ang_col]) % 360.0\n\n    return df\n\n\ndef create_features(df: pd.DataFrame, is_train: bool = True) -> pd.DataFrame:\n    df = df.copy()\n\n    s = df[\"s\"].fillna(0.0)\n    a = df[\"a\"].fillna(0.0)\n    dir_rad = np.deg2rad(df[\"dir\"].fillna(0.0))\n    o_rad = np.deg2rad(df[\"o\"].fillna(0.0))\n\n    df[\"vx\"] = s * np.cos(dir_rad)\n    df[\"vy\"] = s * np.sin(dir_rad)\n    df[\"ax_comp\"] = a * np.cos(dir_rad)\n    df[\"ay_comp\"] = a * np.sin(dir_rad)\n\n    df[\"dir_sin\"] = np.sin(dir_rad)\n    df[\"dir_cos\"] = np.cos(dir_rad)\n    df[\"o_sin\"] = np.sin(o_rad)\n    df[\"o_cos\"] = np.cos(o_rad)\n\n    if \"frame_id\" in df.columns:\n        df[\"frame_offset\"] = df[\"frame_id\"]\n    else:\n        df[\"frame_offset\"] = 0\n\n    df[\"time_offset\"] = df[\"frame_offset\"] / 10.0\n\n    if \"num_frames_output\" in df.columns:\n        nfo = df[\"num_frames_output\"].replace(0, np.nan)\n        df[\"frac_of_flight\"] = (df[\"frame_offset\"] / nfo).clip(lower=0, upper=1)\n        df[\"frac_of_flight\"] = df[\"frac_of_flight\"].fillna(0.0)\n        df[\"frames_left\"] = (nfo - df[\"frame_offset\"]).clip(lower=0).fillna(0.0)\n    else:\n        df[\"frac_of_flight\"] = 0.0\n        df[\"frames_left\"] = 0.0\n\n    df[\"time_to_land\"] = df[\"frames_left\"] / 10.0\n    df[\"remaining_flight_frac\"] = (1.0 - df[\"frac_of_flight\"]).clip(lower=0.0, upper=1.0)\n\n    df[\"dist_to_ball_land\"] = np.sqrt(\n        (df[\"ball_land_x\"] - df[\"x_last\"]) ** 2 +\n        (df[\"ball_land_y\"] - df[\"y_last\"]) ** 2\n    )\n    df[\"angle_to_ball_land\"] = np.arctan2(\n        df[\"ball_land_y\"] - df[\"y_last\"],\n        df[\"ball_land_x\"] - df[\"x_last\"],\n    )\n\n    frames_left_safe = df[\"frames_left\"].replace(0, np.nan)\n    df[\"dist_to_ball_land_per_frame\"] = df[\"dist_to_ball_land\"] / frames_left_safe\n    df[\"dist_to_ball_land_per_frame\"] = (\n        df[\"dist_to_ball_land_per_frame\"]\n        .replace([np.inf, -np.inf], np.nan)\n        .fillna(0.0)\n    )\n\n    df[\"cos_dir_to_ball\"] = np.cos(df[\"angle_to_ball_land\"] - dir_rad)\n    df[\"cos_orient_to_ball\"] = np.cos(df[\"angle_to_ball_land\"] - o_rad)\n\n    play_dir = df.get(\"play_direction\", \"right\").fillna(\"right\")\n    is_left = (play_dir == \"left\").astype(int)\n\n    df[\"x_std\"] = np.where(is_left == 1, 120.0 - df[\"x_last\"], df[\"x_last\"])\n    df[\"ball_land_x_std\"] = np.where(\n        is_left == 1, 120.0 - df[\"ball_land_x\"], df[\"ball_land_x\"]\n    )\n\n    df[\"dx_to_land_std\"] = df[\"ball_land_x_std\"] - df[\"x_std\"]\n    df[\"dy_to_land\"] = df[\"ball_land_y\"] - df[\"y_last\"]\n\n    df[\"dist_to_sideline\"] = np.minimum(df[\"y_last\"], 53.3 - df[\"y_last\"])\n    df[\"dist_to_center\"] = np.abs(df[\"y_last\"] - 53.3 / 2.0)\n\n    yard = df[\"absolute_yardline_number\"].fillna(50.0)\n    yard_100 = yard.clip(lower=0.0, upper=100.0)\n    df[\"yardline_100\"] = yard_100\n    df[\"yardline_norm\"] = yard_100 / 100.0\n    df[\"dist_to_endzone\"] = 100.0 - yard_100\n\n    df[\"dist_to_target_last\"] = np.sqrt(\n        (df[\"target_last_x\"] - df[\"x_last\"]) ** 2 +\n        (df[\"target_last_y\"] - df[\"y_last\"]) ** 2\n    )\n\n    df[\"dx_to_target_last\"] = df[\"target_last_x\"] - df[\"x_last\"]\n    df[\"dy_to_target_last\"] = df[\"target_last_y\"] - df[\"y_last\"]\n    df[\"angle_to_target\"] = np.arctan2(\n        df[\"target_last_y\"] - df[\"y_last\"],\n        df[\"target_last_x\"] - df[\"x_last\"],\n    )\n\n    df[\"cos_dir_to_target\"] = np.cos(df[\"angle_to_target\"] - dir_rad)\n    df[\"cos_orient_to_target\"] = np.cos(df[\"angle_to_target\"] - o_rad)\n\n    df[\"is_target\"] = (df[\"nfl_id\"] == df[\"target_nfl_id\"]).astype(int)\n\n    df[\"x_rel_ball\"] = df[\"x_last\"] - df[\"ball_land_x\"]\n    df[\"y_rel_ball\"] = df[\"y_last\"] - df[\"ball_land_y\"]\n\n    vx = df[\"vx\"]\n    vy = df[\"vy\"]\n\n    ball_cos = np.cos(df[\"angle_to_ball_land\"])\n    ball_sin = np.sin(df[\"angle_to_ball_land\"])\n    df[\"v_toward_ball\"] = vx * ball_cos + vy * ball_sin\n    df[\"v_across_ball\"] = vx * (-ball_sin) + vy * ball_cos\n\n    tgt_cos = np.cos(df[\"angle_to_target\"])\n    tgt_sin = np.sin(df[\"angle_to_target\"])\n    df[\"v_toward_target\"] = vx * tgt_cos + vy * tgt_sin\n    df[\"v_across_target\"] = vx * (-tgt_sin) + vy * tgt_cos\n\n    df[[\"v_toward_ball\", \"v_across_ball\", \"v_toward_target\", \"v_across_target\"]] = (\n        df[[\"v_toward_ball\", \"v_across_ball\", \"v_toward_target\", \"v_across_target\"]]\n        .replace([np.inf, -np.inf], np.nan)\n        .fillna(0.0)\n    )\n\n    h = df[\"player_height\"].replace(0, np.nan)\n    w = df[\"player_weight\"].replace(0, np.nan)\n    df[\"bmi\"] = 703.0 * (w / (h ** 2))\n    df[\"bmi\"] = df[\"bmi\"].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    if is_train and {\"x\", \"y\"}.issubset(df.columns):\n        df[\"dx\"] = df[\"x\"] - df[\"x_last\"]\n        df[\"dy\"] = df[\"y\"] - df[\"y_last\"]\n\n    return df\n\n\ndef prepare_train(df_in: pd.DataFrame, df_out: pd.DataFrame):\n    print(\"\\n[2/4] Preparing training features...\")\n\n    df_out_local = df_out.copy()\n    if \"frame_id\" not in df_out_local.columns:\n        df_out_local[\"frame_id\"] = (\n            df_out_local.groupby([\"game_id\", \"play_id\", \"nfl_id\"]).cumcount()\n        )\n\n    last_obs = prepare_last_obs(df_in)\n    last_obs = add_target_info(last_obs)\n\n    cols_to_keep_existing = [c for c in BASE_COLS if c in last_obs.columns]\n\n    train_raw = df_out_local.merge(\n        last_obs[cols_to_keep_existing],\n        on=[\"game_id\", \"play_id\", \"nfl_id\"],\n        how=\"left\",\n    )\n\n    if \"player_to_predict\" in train_raw.columns:\n        before = len(train_raw)\n        train_raw = train_raw[train_raw[\"player_to_predict\"].astype(bool)].copy()\n        after = len(train_raw)\n        print(f\" Filtered to player_to_predict==True: {before} -> {after} rows\")\n\n    train_main = create_features(train_raw, is_train=True)\n\n    train_mirror_raw = mirror_raw(train_raw)\n    train_mirror = create_features(train_mirror_raw, is_train=True)\n\n    train_tree = pd.concat([train_main, train_mirror], ignore_index=True)\n    print(f\" After symmetry augmentation (tree): {len(train_main)} -> {len(train_tree)} rows\")\n\n    train_gnn = train_main.copy()\n\n    return train_tree, train_gnn\n\n\n# ======================================================================\n# GNN (same as before)\n# ======================================================================\n\nclass GraphDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, num_cols, cat_code_cols):\n        self.num_cols = list(num_cols)\n        self.cat_code_cols = list(cat_code_cols)\n\n        self.X_num = df[self.num_cols].to_numpy(np.float32)\n        if self.cat_code_cols:\n            self.X_cat = df[self.cat_code_cols].to_numpy(np.int64)\n        else:\n            self.X_cat = None\n        self.y = df[[\"dx\", \"dy\"]].to_numpy(np.float32)\n        self.w = (1.0 + TARGET_WEIGHT_GNN * df[\"is_target\"].to_numpy(np.float32)).astype(\n            np.float32\n        )\n\n        if \"frame_id\" in df.columns:\n            gkeys = (\n                df[\"game_id\"].astype(str)\n                + \"_\"\n                + df[\"play_id\"].astype(str)\n                + \"_\"\n                + df[\"frame_id\"].astype(str)\n            )\n        else:\n            gkeys = (\n                df[\"game_id\"].astype(str)\n                + \"_\"\n                + df[\"play_id\"].astype(str)\n            )\n\n        self.graph_ids, _ = pd.factorize(gkeys)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        if self.X_cat is not None:\n            return (\n                self.X_num[idx],\n                self.X_cat[idx],\n                self.y[idx],\n                self.w[idx],\n                int(self.graph_ids[idx]),\n            )\n        else:\n            return (\n                self.X_num[idx],\n                None,\n                self.y[idx],\n                self.w[idx],\n                int(self.graph_ids[idx]),\n            )\n\n\ndef collate_graph(batch):\n    X_num, X_cat, y, w, g_ids = zip(*batch)\n    X_num = torch.from_numpy(np.stack(X_num))\n    if X_cat[0] is not None:\n        X_cat = torch.from_numpy(np.stack(X_cat))\n    else:\n        X_cat = None\n    y = torch.from_numpy(np.stack(y))\n    w = torch.from_numpy(np.stack(w))\n    g_ids = torch.tensor(g_ids, dtype=torch.long)\n    return X_num, X_cat, y, w, g_ids\n\n\nclass GraphAttentionLayer(nn.Module):\n    def __init__(self, in_features, out_features, alpha=0.2):\n        super().__init__()\n        self.W = nn.Linear(in_features, out_features, bias=False)\n        self.a = nn.Linear(2 * out_features, 1, bias=False)\n        self.leakyrelu = nn.LeakyReLU(alpha)\n\n    def forward(self, h, adj):\n        Wh = self.W(h)\n        N = Wh.size(0)\n\n        Wh_i = Wh.unsqueeze(1).repeat(1, N, 1)\n        Wh_j = Wh.unsqueeze(0).repeat(N, 1, 1)\n\n        e_input = torch.cat([Wh_i, Wh_j], dim=2)\n        e = self.leakyrelu(self.a(e_input).squeeze(2))\n\n        e = e.masked_fill(adj == 0, float(\"-inf\"))\n        attention = torch.softmax(e, dim=1)\n\n        h_prime = attention @ Wh\n        return h_prime\n\n\nclass MultiHeadGAT(nn.Module):\n    def __init__(self, in_features, out_features, num_heads=2, alpha=0.2):\n        super().__init__()\n        assert out_features % num_heads == 0\n        head_dim = out_features // num_heads\n        self.heads = nn.ModuleList(\n            [GraphAttentionLayer(in_features, head_dim, alpha=alpha) for _ in range(num_heads)]\n        )\n        self.out_proj = nn.Linear(out_features, out_features)\n\n    def forward(self, h, adj):\n        head_outs = [head(h, adj) for head in self.heads]\n        h_cat = torch.cat(head_outs, dim=1)\n        out = self.out_proj(h_cat)\n        return F.relu(out)\n\n\nclass GCNLayer(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.W = nn.Linear(in_features, out_features, bias=False)\n\n    def forward(self, h, adj):\n        N = adj.size(0)\n        I = torch.eye(N, device=adj.device)\n        A_hat = adj + I\n        deg = A_hat.sum(dim=1)\n        deg_inv_sqrt = deg.pow(-0.5)\n        deg_inv_sqrt[torch.isinf(deg_inv_sqrt)] = 0.0\n        D_inv_sqrt = torch.diag(deg_inv_sqrt)\n        A_norm = D_inv_sqrt @ A_hat @ D_inv_sqrt\n\n        hW = self.W(h)\n        out = A_norm @ hW\n        return F.relu(out)\n\n\nclass GATGCNModel(nn.Module):\n    def __init__(\n        self,\n        num_num_features: int,\n        num_cat_features: int,\n        cat_cardinalities,\n        hidden_dim: int = 384,\n        emb_dim: int = 16,\n        gat_heads: int = 2,\n        dropout: float = 0.1,\n    ):\n        super().__init__()\n        self.num_num_features = num_num_features\n        self.num_cat_features = num_cat_features\n\n        self.emb_layers = nn.ModuleList()\n        total_emb_dim = 0\n        if num_cat_features > 0:\n            for card in cat_cardinalities:\n                dim = min(emb_dim, (card + 1) // 2)\n                self.emb_layers.append(nn.Embedding(card, dim))\n                total_emb_dim += dim\n\n        self.num_linear = nn.Linear(num_num_features, hidden_dim)\n        self.cat_linear = nn.Linear(total_emb_dim, hidden_dim) if total_emb_dim > 0 else None\n\n        self.gat1 = MultiHeadGAT(hidden_dim, hidden_dim, num_heads=gat_heads)\n        self.gat2 = MultiHeadGAT(hidden_dim, hidden_dim, num_heads=gat_heads)\n        self.gcn1 = GCNLayer(hidden_dim, hidden_dim)\n        self.gcn2 = GCNLayer(hidden_dim, hidden_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n        self.out_mlp = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, 2),\n        )\n\n    def forward(self, x_num, x_cat, adj):\n        h_num = self.num_linear(x_num)\n\n        if self.emb_layers and x_cat is not None:\n            embs = []\n            for i, emb_layer in enumerate(self.emb_layers):\n                embs.append(emb_layer(x_cat[:, i]))\n            cat_emb = torch.cat(embs, dim=1)\n            h_cat = self.cat_linear(cat_emb) if self.cat_linear is not None else 0.0\n            h = F.relu(h_num + h_cat)\n        else:\n            h = F.relu(h_num)\n\n        h = self.gat1(h, adj)\n        h = self.dropout(h)\n        h = self.gat2(h, adj)\n        h = self.dropout(h)\n        h = self.gcn1(h, adj)\n        h = self.dropout(h)\n        h = self.gcn2(h, adj)\n        h = self.dropout(h)\n\n        out = self.out_mlp(h)\n        return out\n\n\ndef train_gnn_model(train_gnn: pd.DataFrame):\n    global GNN_MODEL\n\n    print(\" Training GAT+GCN model (dx, dy)...\")\n\n    for col in GNN_NUM_FEATS:\n        if col not in train_gnn.columns:\n            train_gnn[col] = 0.0\n\n    for c in CAT_FEATS:\n        cats = CAT_CATEGORY_MAPS[c]\n        codes = pd.Categorical(train_gnn[c], categories=cats).codes\n        codes = np.where(codes < 0, len(cats), codes)\n        train_gnn[f\"{c}_cat\"] = codes.astype(\"int64\")\n\n    train_gnn_scaled = train_gnn.copy()\n    train_gnn_scaled[GNN_NUM_FEATS] = (\n        (train_gnn_scaled[GNN_NUM_FEATS] - GNN_NUM_MEAN) / GNN_NUM_STD\n    ).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    dataset = GraphDataset(train_gnn_scaled, GNN_NUM_FEATS, GNN_CAT_CODE_COLS)\n    loader = DataLoader(\n        dataset,\n        batch_size=GNN_BATCH_SIZE,\n        shuffle=True,\n        collate_fn=collate_graph,\n        drop_last=False,\n    )\n\n    cat_cardinalities = [CAT_CARD_SIZES[c] for c in CAT_FEATS]\n\n    model = GATGCNModel(\n        num_num_features=len(GNN_NUM_FEATS),\n        num_cat_features=len(GNN_CAT_CODE_COLS),\n        cat_cardinalities=cat_cardinalities,\n        hidden_dim=GNN_HIDDEN_DIM,\n        emb_dim=GNN_EMB_DIM,\n        gat_heads=GNN_GAT_HEADS,\n        dropout=GNN_DROPOUT,\n    ).to(DEVICE)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    loss_fn = nn.MSELoss(reduction=\"none\")\n\n    model.train()\n    for epoch in range(GNN_EPOCHS):\n        running_loss = 0.0\n        n_samples = 0\n        for X_num, X_cat, y, w, g_ids in loader:\n            X_num = X_num.to(DEVICE)\n            X_cat = X_cat.to(DEVICE) if X_cat is not None else None\n            y = y.to(DEVICE)\n            w = w.to(DEVICE)\n            g_ids = g_ids.to(DEVICE)\n\n            adj = (g_ids.unsqueeze(1) == g_ids.unsqueeze(0)).float()\n\n            optimizer.zero_grad()\n            pred = model(X_num, X_cat, adj)\n            loss_per_coord = loss_fn(pred, y).mean(dim=1)\n            loss = (loss_per_coord * w).mean()\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * y.size(0)\n            n_samples += y.size(0)\n\n        avg_loss = running_loss / max(1, n_samples)\n        print(f\"  GNN epoch {epoch + 1}/{GNN_EPOCHS}: loss = {avg_loss:.6f}\")\n\n    GNN_MODEL = model\n    print(\"✓ GNN model trained\")\n    return model\n\n\n# ======================================================================\n# BAYESIAN OPTIMIZATION: LGBM & XGB\n# ======================================================================\n\ndef tune_lgbm_bayes(train_tree: pd.DataFrame, max_train_rows: int = 300_000):\n    print(\"\\n[2.5] Bayesian hyperparameter tuning for LGBM...\")\n\n    if len(train_tree) > max_train_rows:\n        work_df = train_tree.sample(n=max_train_rows, random_state=RANDOM_STATE).reset_index(drop=True)\n        print(f\"  Subsampled train_tree: {len(train_tree)} -> {len(work_df)} rows for tuning\")\n    else:\n        work_df = train_tree.reset_index(drop=True)\n\n    train_df, val_df = make_holdout_split(work_df, val_frac=0.2, random_state=RANDOM_STATE)\n\n    for col in FEATURES:\n        if col not in train_df.columns:\n            train_df[col] = 0.0\n            val_df[col] = 0.0\n    for c in CAT_FEATS:\n        if c not in train_df.columns:\n            train_df[c] = \"unknown\"\n            val_df[c] = \"unknown\"\n        train_df[c] = train_df[c].astype(\"category\")\n        val_df[c] = val_df[c].astype(\"category\")\n\n    X_tr = train_df[FEATURES + CAT_FEATS].copy()\n    X_va = val_df[FEATURES + CAT_FEATS].copy()\n\n    y_tr_dx = train_df[\"dx\"].values\n    y_tr_dy = train_df[\"dy\"].values\n    y_va_x = val_df[\"x\"].values\n    y_va_y = val_df[\"y\"].values\n\n    x_last_va = val_df[\"x_last\"].values\n    y_last_va = val_df[\"y_last\"].values\n\n    w_tr = (1.0 + TARGET_WEIGHT_TREE * train_df[\"is_target\"].values.astype(np.float32)).astype(np.float32)\n\n    base_params = dict(\n        objective=\"regression\",\n        boosting_type=\"gbdt\",\n        min_data_in_leaf=50,\n        feature_fraction=0.9,\n        bagging_fraction=0.9,\n        bagging_freq=1,\n        verbosity=-1,\n        random_state=RANDOM_STATE,\n    )\n\n    def objective(trial):\n        num_leaves = trial.suggest_int(\"num_leaves\", 63, 511)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.15, log=True)\n        n_estimators = trial.suggest_int(\"n_estimators\", 600, 1600)\n        feature_fraction = trial.suggest_float(\"feature_fraction\", 0.7, 1.0)\n        bagging_fraction = trial.suggest_float(\"bagging_fraction\", 0.7, 1.0)\n        bagging_freq = trial.suggest_int(\"bagging_freq\", 1, 5)\n\n        params = dict(base_params)\n        params.update(\n            dict(\n                num_leaves=num_leaves,\n                learning_rate=learning_rate,\n                n_estimators=n_estimators,\n                feature_fraction=feature_fraction,\n                bagging_fraction=bagging_fraction,\n                bagging_freq=bagging_freq,\n            )\n        )\n\n        mdl_dx = LGBMRegressor(**params)\n        mdl_dx.fit(X_tr, y_tr_dx, categorical_feature=CAT_FEATS, sample_weight=w_tr)\n\n        mdl_dy = LGBMRegressor(**params)\n        mdl_dy.fit(X_tr, train_df[\"dy\"].values, categorical_feature=CAT_FEATS, sample_weight=w_tr)\n\n        pred_dx = mdl_dx.predict(X_va)\n        pred_dy = mdl_dy.predict(X_va)\n\n        x_pred = x_last_va + pred_dx\n        y_pred = y_last_va + pred_dy\n        rmse_val = rmse_xy(y_va_x, y_va_y, x_pred, y_pred)\n        return rmse_val\n\n    if HAS_OPTUNA:\n        study = optuna.create_study(direction=\"minimize\")\n        study.optimize(objective, n_trials=N_TRIALS_LGBM, show_progress_bar=True)\n        best_params = study.best_params\n        print(f\"  Best LGBM params (Bayes): {best_params}, val RMSE: {study.best_value:.4f}\")\n    else:\n        # fallback: simple random search over a small space\n        print(\"  Optuna not available -> falling back to random search LGBM\")\n        search_space = [\n            {\"num_leaves\": 63, \"learning_rate\": 0.05, \"n_estimators\": 800},\n            {\"num_leaves\": 127, \"learning_rate\": 0.05, \"n_estimators\": 1000},\n            {\"num_leaves\": 255, \"learning_rate\": 0.05, \"n_estimators\": 1200},\n            {\"num_leaves\": 255, \"learning_rate\": 0.07, \"n_estimators\": 1000},\n            {\"num_leaves\": 511, \"learning_rate\": 0.05, \"n_estimators\": 1400},\n        ]\n        best_rmse = float(\"inf\")\n        best_params = None\n        for cfg in search_space:\n            params = dict(base_params)\n            params.update(cfg)\n            print(\"   Trying\", cfg)\n            mdl_dx = LGBMRegressor(**params)\n            mdl_dx.fit(X_tr, y_tr_dx, categorical_feature=CAT_FEATS, sample_weight=w_tr)\n            mdl_dy = LGBMRegressor(**params)\n            mdl_dy.fit(X_tr, train_df[\"dy\"].values, categorical_feature=CAT_FEATS, sample_weight=w_tr)\n\n            pred_dx = mdl_dx.predict(X_va)\n            pred_dy = mdl_dy.predict(X_va)\n            x_pred = x_last_va + pred_dx\n            y_pred = y_last_va + pred_dy\n            rm = rmse_xy(y_va_x, y_va_y, x_pred, y_pred)\n            print(f\"    -> RMSE={rm:.4f}\")\n            if rm < best_rmse:\n                best_rmse = rm\n                best_params = cfg\n        print(f\"  Best LGBM params (fallback): {best_params}, val RMSE: {best_rmse:.4f}\")\n\n    return best_params\n\n\ndef tune_xgb_bayes(train_tree: pd.DataFrame, max_train_rows: int = 300_000):\n    print(\"\\n[2.6] Bayesian hyperparameter tuning for XGBoost...\")\n\n    if len(train_tree) > max_train_rows:\n        work_df = train_tree.sample(n=max_train_rows, random_state=RANDOM_STATE).reset_index(drop=True)\n        print(f\"  Subsampled train_tree: {len(train_tree)} -> {len(work_df)} rows for tuning\")\n    else:\n        work_df = train_tree.reset_index(drop=True)\n\n    train_df, val_df = make_holdout_split(work_df, val_frac=0.2, random_state=RANDOM_STATE)\n\n    for col in FEATURES:\n        if col not in train_df.columns:\n            train_df[col] = 0.0\n            val_df[col] = 0.0\n    for c in CAT_FEATS:\n        if c not in train_df.columns:\n            train_df[c] = \"unknown\"\n            val_df[c] = \"unknown\"\n        train_df[c] = train_df[c].astype(\"category\")\n        val_df[c] = val_df[c].astype(\"category\")\n\n    num_cols = FEATURES\n\n    X_tr = train_df[FEATURES + CAT_FEATS].copy()\n    X_va = val_df[FEATURES + CAT_FEATS].copy()\n\n    X_tr[num_cols] = X_tr[num_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    X_va[num_cols] = X_va[num_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    for c in CAT_FEATS:\n        X_tr[c] = X_tr[c].astype(\"category\")\n        X_va[c] = X_va[c].astype(\"category\")\n\n    y_tr_dx = train_df[\"dx\"].values\n    y_tr_dy = train_df[\"dy\"].values\n    y_va_x = val_df[\"x\"].values\n    y_va_y = val_df[\"y\"].values\n\n    x_last_va = val_df[\"x_last\"].values\n    y_last_va = val_df[\"y_last\"].values\n\n    w_tr = (1.0 + TARGET_WEIGHT_TREE * train_df[\"is_target\"].values.astype(np.float32)).astype(np.float32)\n\n    base_params = dict(\n        objective=\"reg:squarederror\",\n        tree_method=\"hist\",\n        enable_categorical=True,\n        n_jobs=-1,\n        reg_lambda=1.0,\n        random_state=RANDOM_STATE,\n    )\n\n    def objective(trial):\n        max_depth = trial.suggest_int(\"max_depth\", 5, 12)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.15, log=True)\n        n_estimators = trial.suggest_int(\"n_estimators\", 400, 900)\n        subsample = trial.suggest_float(\"subsample\", 0.7, 1.0)\n        colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.7, 1.0)\n\n        params = dict(base_params)\n        params.update(\n            dict(\n                max_depth=max_depth,\n                learning_rate=learning_rate,\n                n_estimators=n_estimators,\n                subsample=subsample,\n                colsample_bytree=colsample_bytree,\n            )\n        )\n\n        mdl_dx = xgb.XGBRegressor(**params)\n        mdl_dx.fit(X_tr, y_tr_dx, sample_weight=w_tr)\n\n        mdl_dy = xgb.XGBRegressor(**params)\n        mdl_dy.fit(X_tr, y_tr_dy, sample_weight=w_tr)\n\n        pred_dx = mdl_dx.predict(X_va)\n        pred_dy = mdl_dy.predict(X_va)\n\n        x_pred = x_last_va + pred_dx\n        y_pred = y_last_va + pred_dy\n        rmse_val = rmse_xy(y_va_x, y_va_y, x_pred, y_pred)\n        return rmse_val\n\n    if HAS_OPTUNA:\n        study = optuna.create_study(direction=\"minimize\")\n        study.optimize(objective, n_trials=N_TRIALS_XGB, show_progress_bar=True)\n        best_params = study.best_params\n        print(f\"  Best XGB params (Bayes): {best_params}, val RMSE: {study.best_value:.4f}\")\n    else:\n        print(\"  Optuna not available -> falling back to random search XGB\")\n        search_space = [\n            {\"max_depth\": 8, \"learning_rate\": 0.05, \"n_estimators\": 600, \"subsample\": 0.9, \"colsample_bytree\": 0.9},\n            {\"max_depth\": 10, \"learning_rate\": 0.05, \"n_estimators\": 700, \"subsample\": 0.85, \"colsample_bytree\": 0.8},\n            {\"max_depth\": 10, \"learning_rate\": 0.07, \"n_estimators\": 600, \"subsample\": 0.9, \"colsample_bytree\": 0.9},\n        ]\n        best_rmse = float(\"inf\")\n        best_params = None\n        for cfg in search_space:\n            params = dict(base_params)\n            params.update(cfg)\n            print(\"   Trying\", cfg)\n            mdl_dx = xgb.XGBRegressor(**params)\n            mdl_dx.fit(X_tr, y_tr_dx, sample_weight=w_tr)\n            mdl_dy = xgb.XGBRegressor(**params)\n            mdl_dy.fit(X_tr, y_tr_dy, sample_weight=w_tr)\n\n            pred_dx = mdl_dx.predict(X_va)\n            pred_dy = mdl_dy.predict(X_va)\n            x_pred = x_last_va + pred_dx\n            y_pred = y_last_va + pred_dy\n            rm = rmse_xy(y_va_x, y_va_y, x_pred, y_pred)\n            print(f\"    -> RMSE={rm:.4f}\")\n            if rm < best_rmse:\n                best_rmse = rm\n                best_params = cfg\n        print(f\"  Best XGB params (fallback): {best_params}, val RMSE: {best_rmse:.4f}\")\n\n    return best_params\n\n\n# ======================================================================\n# ENSEMBLE WEIGHTS TUNING & SAVE\n# ======================================================================\n\ndef tune_ensemble_weights(train_tree: pd.DataFrame, max_rows: int = 50_000):\n    global ENSEMBLE_WEIGHTS\n\n    print(\"\\n[4/4] Tuning ensemble weights (LGBM / XGB / GNN)...\")\n\n    if len(train_tree) > max_rows:\n        work_df = train_tree.sample(n=max_rows, random_state=RANDOM_STATE).reset_index(drop=True)\n        print(f\"  Subsampled train_tree: {len(train_tree)} -> {len(work_df)} rows for ensemble tuning\")\n    else:\n        work_df = train_tree.reset_index(drop=True)\n\n    for col in FEATURES:\n        if col not in work_df.columns:\n            work_df[col] = 0.0\n    for c in CAT_FEATS:\n        if c not in work_df.columns:\n            work_df[c] = \"unknown\"\n        work_df[c] = work_df[c].astype(\"category\")\n\n    X_tree = work_df[FEATURES + CAT_FEATS].copy()\n    for c in CAT_FEATS:\n        X_tree[c] = X_tree[c].astype(\"category\")\n\n    # LGBM preds\n    if LGBM_MODELS_DX:\n        pred_dx_lgb_list = [m.predict(X_tree) for m in LGBM_MODELS_DX]\n        pred_dy_lgb_list = [m.predict(X_tree) for m in LGBM_MODELS_DY]\n        pred_dx_lgb = np.mean(pred_dx_lgb_list, axis=0)\n        pred_dy_lgb = np.mean(pred_dy_lgb_list, axis=0)\n    else:\n        pred_dx_lgb = np.zeros(len(work_df), dtype=np.float32)\n        pred_dy_lgb = np.zeros(len(work_df), dtype=np.float32)\n\n    # XGB preds\n    num_cols = FEATURES\n    X_xgb = work_df[FEATURES + CAT_FEATS].copy()\n    X_xgb[num_cols] = X_xgb[num_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    for c in CAT_FEATS:\n        X_xgb[c] = X_xgb[c].astype(\"category\")\n\n    if XGB_MODELS_DX:\n        pred_dx_xgb_list = [m.predict(X_xgb) for m in XGB_MODELS_DX]\n        pred_dy_xgb_list = [m.predict(X_xgb) for m in XGB_MODELS_DY]\n        pred_dx_xgb = np.mean(pred_dx_xgb_list, axis=0)\n        pred_dy_xgb = np.mean(pred_dy_xgb_list, axis=0)\n    else:\n        pred_dx_xgb = np.zeros(len(work_df), dtype=np.float32)\n        pred_dy_xgb = np.zeros(len(work_df), dtype=np.float32)\n\n    # GNN preds (optional; we know it's usually weaker)\n    if GNN_MODEL is not None and GNN_NUM_MEAN is not None and GNN_NUM_STD is not None:\n        df_g = work_df.copy()\n        for c in CAT_FEATS:\n            cats = CAT_CATEGORY_MAPS[c]\n            codes = pd.Categorical(df_g[c], categories=cats).codes\n            codes = np.where(codes < 0, len(cats), codes)\n            df_g[f\"{c}_cat\"] = codes.astype(\"int64\")\n\n        for col in GNN_NUM_FEATS:\n            if col not in df_g.columns:\n                df_g[col] = 0.0\n\n        X_num_df = df_g[GNN_NUM_FEATS].copy()\n        X_num_df = (\n            (X_num_df - GNN_NUM_MEAN) / GNN_NUM_STD\n        ).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n        X_num = X_num_df.to_numpy(np.float32)\n        X_cat = df_g[GNN_CAT_CODE_COLS].to_numpy(np.int64)\n\n        if \"frame_id\" in df_g.columns:\n            gkeys = (\n                df_g[\"game_id\"].astype(str)\n                + \"_\"\n                + df_g[\"play_id\"].astype(str)\n                + \"_\"\n                + df_g[\"frame_id\"].astype(str)\n            )\n        else:\n            gkeys = (\n                df_g[\"game_id\"].astype(str)\n                + \"_\"\n                + df_g[\"play_id\"].astype(str)\n            )\n        g_ids, _ = pd.factorize(gkeys)\n\n        unique_graphs = np.unique(g_ids)\n        pred_dx_gnn = np.zeros(len(df_g), dtype=np.float32)\n        pred_dy_gnn = np.zeros(len(df_g), dtype=np.float32)\n\n        GNN_MODEL.eval()\n        with torch.no_grad():\n            for graph_id in unique_graphs:\n                mask = g_ids == graph_id\n                if mask.sum() == 0:\n                    continue\n\n                X_num_batch = torch.from_numpy(X_num[mask]).to(DEVICE)\n                X_cat_batch = torch.from_numpy(X_cat[mask]).to(DEVICE)\n\n                n_nodes = X_num_batch.size(0)\n                adj_batch = torch.ones(n_nodes, n_nodes, device=DEVICE)\n\n                pred_nn_batch = GNN_MODEL(X_num_batch, X_cat_batch, adj_batch).cpu().numpy()\n                pred_dx_gnn[mask] = pred_nn_batch[:, 0]\n                pred_dy_gnn[mask] = pred_nn_batch[:, 1]\n\n        print(f\"  Processed {len(unique_graphs)} graphs for GNN predictions\")\n    else:\n        pred_dx_gnn = np.zeros(len(work_df), dtype=np.float32)\n        pred_dy_gnn = np.zeros(len(work_df), dtype=np.float32)\n\n    x_last = work_df[\"x_last\"].values\n    y_last = work_df[\"y_last\"].values\n    x_true = work_df[\"x\"].values\n    y_true = work_df[\"y\"].values\n\n    # First, measure single-model RMSEs\n    rm_lgb = rmse_xy(x_true, y_true, x_last + pred_dx_lgb, y_last + pred_dy_lgb)\n    rm_xgb = rmse_xy(x_true, y_true, x_last + pred_dx_xgb, y_last + pred_dy_xgb)\n    rm_gnn = rmse_xy(x_true, y_true, x_last + pred_dx_gnn, y_last + pred_dy_gnn)\n    print(f\"  Single-model RMSE [LGBM] = {rm_lgb:.4f}\")\n    print(f\"  Single-model RMSE [XGB ] = {rm_xgb:.4f}\")\n    print(f\"  Single-model RMSE [GNN ] = {rm_gnn:.4f}\")\n\n    # Search 3-model ensemble weights on a grid (step=0.1)\n    print(\"  Searching weights for 3-model ensemble (LGBM+XGB+GNN) with step=0.1 ...\")\n\n    best_rmse = float(\"inf\")\n    best_w = (1.0, 0.0, 0.0)\n    step = 0.1\n    w_values = np.arange(0.0, 1.0 + 1e-9, step)\n\n    for w_l in w_values:\n        for w_x in w_values:\n            w_g = 1.0 - w_l - w_x\n            if w_g < -1e-9 or w_g > 1.0:\n                continue\n\n            dx = w_l * pred_dx_lgb + w_x * pred_dx_xgb + w_g * pred_dx_gnn\n            dy = w_l * pred_dy_lgb + w_x * pred_dy_xgb + w_g * pred_dy_gnn\n            x_pred = x_last + dx\n            y_pred = y_last + dy\n            rm = rmse_xy(x_true, y_true, x_pred, y_pred)\n            if rm < best_rmse:\n                best_rmse = rm\n                best_w = (w_l, w_x, w_g)\n\n    ENSEMBLE_WEIGHTS = {\"lgbm\": best_w[0], \"xgb\": best_w[1], \"gnn\": best_w[2]}\n    print(\n        f\"  Best ensemble weights: LGBM={best_w[0]:.2f}, \"\n        f\"XGB={best_w[1]:.2f}, GNN={best_w[2]:.2f}, RMSE={best_rmse:.4f}\"\n    )\n\n    return ENSEMBLE_WEIGHTS\n\n\ndef save_trained_models(output_dir: str = \"models\"):\n    os.makedirs(output_dir, exist_ok=True)\n    out_dir = Path(output_dir)\n\n    for i, m in enumerate(LGBM_MODELS_DX):\n        joblib.dump(m, out_dir / f\"lgbm_dx_{i}.pkl\")\n    for i, m in enumerate(LGBM_MODELS_DY):\n        joblib.dump(m, out_dir / f\"lgbm_dy_{i}.pkl\")\n    print(f\"  Saved {len(LGBM_MODELS_DX)} LGBM dx and {len(LGBM_MODELS_DY)} LGBM dy models to {out_dir}\")\n\n    for i, m in enumerate(XGB_MODELS_DX):\n        joblib.dump(m, out_dir / f\"xgb_dx_{i}.pkl\")\n    for i, m in enumerate(XGB_MODELS_DY):\n        joblib.dump(m, out_dir / f\"xgb_dy_{i}.pkl\")\n    print(f\"  Saved {len(XGB_MODELS_DX)} XGB dx and {len(XGB_MODELS_DY)} XGB dy models to {out_dir}\")\n\n    if GNN_MODEL is not None:\n        torch.save(GNN_MODEL.state_dict(), out_dir / \"gnn_model.pth\")\n        print(\"  Saved GNN model state_dict to gnn_model.pth\")\n\n    meta = dict(\n        FEATURES=FEATURES,\n        CAT_FEATS=CAT_FEATS,\n        GNN_NUM_FEATS=GNN_NUM_FEATS,\n        CAT_CATEGORY_MAPS=CAT_CATEGORY_MAPS,\n        CAT_CARD_SIZES=CAT_CARD_SIZES,\n        GNN_NUM_MEAN=GNN_NUM_MEAN.to_dict() if GNN_NUM_MEAN is not None else None,\n        GNN_NUM_STD=GNN_NUM_STD.to_dict() if GNN_NUM_STD is not None else None,\n        ENSEMBLE_WEIGHTS=ENSEMBLE_WEIGHTS,\n        TARGET_WEIGHT_TREE=TARGET_WEIGHT_TREE,\n        TARGET_WEIGHT_GNN=TARGET_WEIGHT_GNN,\n    )\n    joblib.dump(meta, out_dir / \"meta.pkl\")\n    print(\"  Saved metadata to meta.pkl\")\n\n\n# ======================================================================\n# MAIN TRAINING DRIVER\n# ======================================================================\n\ndef train_models(train_tree: pd.DataFrame, train_gnn: pd.DataFrame):\n    global CAT_CATEGORY_MAPS, CAT_CARD_SIZES\n    global GNN_NUM_MEAN, GNN_NUM_STD\n    global LGBM_MODELS_DX, LGBM_MODELS_DY\n    global XGB_MODELS_DX, XGB_MODELS_DY\n\n    print(\"\\n[3/4] Training models...\")\n\n    # Ensure features and cats present\n    for col in FEATURES:\n        if col not in train_tree.columns:\n            train_tree[col] = 0.0\n        if col not in train_gnn.columns:\n            train_gnn[col] = 0.0\n    for col in CAT_FEATS:\n        if col not in train_tree.columns:\n            train_tree[col] = \"unknown\"\n        if col not in train_gnn.columns:\n            train_gnn[col] = \"unknown\"\n\n    for c in CAT_FEATS:\n        train_tree[c] = train_tree[c].astype(\"category\")\n        cats = list(train_tree[c].cat.categories)\n        CAT_CATEGORY_MAPS[c] = cats\n        CAT_CARD_SIZES[c] = len(cats) + 1\n\n    # ---- 1) Bayesian LGBM tuning ----\n    best_cfg_lgbm = tune_lgbm_bayes(train_tree)\n    if best_cfg_lgbm is None:\n        best_cfg_lgbm = {\"num_leaves\": 255, \"learning_rate\": 0.05, \"n_estimators\": 1200}\n        print(\"  WARNING: LGBM tuning failed, using fallback:\", best_cfg_lgbm)\n\n    # ---- 2) Bayesian XGB tuning ----\n    best_cfg_xgb = tune_xgb_bayes(train_tree)\n    if best_cfg_xgb is None:\n        best_cfg_xgb = {\n            \"max_depth\": 10,\n            \"learning_rate\": 0.05,\n            \"n_estimators\": 700,\n            \"subsample\": 0.85,\n            \"colsample_bytree\": 0.8,\n        }\n        print(\"  WARNING: XGB tuning failed, using fallback:\", best_cfg_xgb)\n\n    # ---- 3) Train LGBM ensemble on full train_tree ----\n    X_tree = train_tree[FEATURES + CAT_FEATS].copy()\n    for c in CAT_FEATS:\n        X_tree[c] = X_tree[c].astype(\"category\")\n\n    y_dx = train_tree[\"dx\"].values\n    y_dy = train_tree[\"dy\"].values\n    sample_weight = (1.0 + TARGET_WEIGHT_TREE * train_tree[\"is_target\"].values).astype(np.float32)\n\n    base_params_lgbm = dict(\n        objective=\"regression\",\n        boosting_type=\"gbdt\",\n        min_data_in_leaf=50,\n        feature_fraction=best_cfg_lgbm.get(\"feature_fraction\", 0.9),\n        bagging_fraction=best_cfg_lgbm.get(\"bagging_fraction\", 0.9),\n        bagging_freq=best_cfg_lgbm.get(\"bagging_freq\", 1),\n        n_estimators=best_cfg_lgbm[\"n_estimators\"],\n        learning_rate=best_cfg_lgbm[\"learning_rate\"],\n        num_leaves=best_cfg_lgbm[\"num_leaves\"],\n        verbosity=-1,\n    )\n\n    LGBM_MODELS_DX = []\n    LGBM_MODELS_DY = []\n\n    print(f\"\\n  Training LGBM ensemble with tuned params: {base_params_lgbm}\")\n    for m in range(LGBM_N_MODELS):\n        seed = RANDOM_STATE + m\n        params = dict(base_params_lgbm)\n        params[\"random_state\"] = seed\n\n        print(f\"   -> LGBM model {m + 1}/{LGBM_N_MODELS} for dx...\")\n        model_dx = LGBMRegressor(**params)\n        model_dx.fit(X_tree, y_dx, categorical_feature=CAT_FEATS, sample_weight=sample_weight)\n        LGBM_MODELS_DX.append(model_dx)\n\n        print(f\"   -> LGBM model {m + 1}/{LGBM_N_MODELS} for dy...\")\n        model_dy = LGBMRegressor(**params)\n        model_dy.fit(X_tree, y_dy, categorical_feature=CAT_FEATS, sample_weight=sample_weight)\n        LGBM_MODELS_DY.append(model_dy)\n\n    print(\"✓ LGBM ensemble trained on full dataset\")\n\n    # ---- 4) Train XGBoost on full train_tree ----\n    num_cols = FEATURES\n    X_xgb_full = train_tree[FEATURES + CAT_FEATS].copy()\n    X_xgb_full[num_cols] = X_xgb_full[num_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    for c in CAT_FEATS:\n        X_xgb_full[c] = X_xgb_full[c].astype(\"category\")\n\n    base_params_xgb = dict(\n        objective=\"reg:squarederror\",\n        tree_method=\"hist\",\n        enable_categorical=True,\n        n_jobs=-1,\n        reg_lambda=1.0,\n        random_state=RANDOM_STATE,\n    )\n    base_params_xgb.update(best_cfg_xgb)\n\n    print(f\"\\n  Training XGBoost models with tuned params: {base_params_xgb}\")\n    XGB_MODELS_DX = []\n    XGB_MODELS_DY = []\n\n    print(\"   -> XGB model for dx...\")\n    xgb_dx = xgb.XGBRegressor(**base_params_xgb)\n    xgb_dx.fit(X_xgb_full, y_dx, sample_weight=sample_weight)\n    XGB_MODELS_DX.append(xgb_dx)\n\n    print(\"   -> XGB model for dy...\")\n    xgb_dy = xgb.XGBRegressor(**base_params_xgb)\n    xgb_dy.fit(X_xgb_full, y_dy, sample_weight=sample_weight)\n    XGB_MODELS_DY.append(xgb_dy)\n\n    print(\"✓ XGBoost models trained on full dataset\")\n\n    # ---- 5) GNN normalization + training ----\n    global GNN_MODEL\n    GNN_NUM_MEAN = train_gnn[GNN_NUM_FEATS].mean()\n    GNN_NUM_STD = train_gnn[GNN_NUM_FEATS].std().replace(0, 1.0).fillna(1.0)\n\n    if TRAIN_GNN:\n        train_gnn_model(train_gnn)\n    else:\n        print(\"  TRAIN_GNN=False -> skipping GNN training\")\n\n    # ---- 6) Tune ensemble weights ----\n    tune_ensemble_weights(train_tree)\n\n    # ---- 7) Save all models + meta ----\n    save_trained_models(output_dir=\"models\")\n\n\n# ======================================================================\n# ENTRYPOINT\n# ======================================================================\n\nif __name__ == \"__main__\":\n    df_in, df_out = load_train(DATA_DIR)\n    train_tree_df, train_gnn_df = prepare_train(df_in, df_out)\n    train_models(train_tree_df, train_gnn_df)\n    print(\"\\nTraining finished. Models + meta saved in ./models\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T06:04:24.530120Z","iopub.execute_input":"2025-12-01T06:04:24.530821Z","iopub.status.idle":"2025-12-01T09:22:41.719498Z","shell.execute_reply.started":"2025-12-01T06:04:24.530785Z","shell.execute_reply":"2025-12-01T09:22:41.718767Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nNFL BIG DATA BOWL 2026 - Training v2 (Bayesian LGBM + XGB + GNN)\n======================================================================\nDEVICE: cuda\n\n[1/4] Loading training inputs/outputs...\n Week 01: input (285714, 23), output (32088, 6)\n Week 02: input (288586, 23), output (32180, 6)\n Week 03: input (297757, 23), output (36080, 6)\n Week 04: input (272475, 23), output (30147, 6)\n Week 05: input (254779, 23), output (29319, 6)\n Week 06: input (270676, 23), output (31162, 6)\n Week 07: input (233597, 23), output (27443, 6)\n Week 08: input (281011, 23), output (33017, 6)\n Week 09: input (252796, 23), output (28291, 6)\n Week 10: input (260372, 23), output (29008, 6)\n Week 11: input (243413, 23), output (27623, 6)\n Week 12: input (294940, 23), output (32156, 6)\n Week 13: input (233755, 23), output (29568, 6)\n Week 14: input (279972, 23), output (32873, 6)\n Week 15: input (281820, 23), output (32715, 6)\n Week 16: input (316417, 23), output (36508, 6)\n Week 17: input (277582, 23), output (33076, 6)\n Week 18: input (254917, 23), output (29682, 6)\nTrain inputs: (4880579, 23) train outputs: (562936, 6)\n\n[2/4] Preparing training features...\n Filtered to player_to_predict==True: 562936 -> 562936 rows\n After symmetry augmentation (tree): 562936 -> 1125872 rows\n\n[3/4] Training models...\n\n[2.5] Bayesian hyperparameter tuning for LGBM...\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-12-01 06:05:26,717] A new study created in memory with name: no-name-580c64b9-4d68-415d-89df-ef94db0d95c3\n","output_type":"stream"},{"name":"stdout","text":"  Subsampled train_tree: 1125872 -> 300000 rows for tuning\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a21d66dcf1b3484c9cf2e46875784b38"}},"metadata":{}},{"name":"stdout","text":"[I 2025-12-01 06:08:14,153] Trial 0 finished with value: 0.4538571514173923 and parameters: {'num_leaves': 238, 'learning_rate': 0.08735122575389254, 'n_estimators': 1501, 'feature_fraction': 0.8814870584760316, 'bagging_fraction': 0.9620499827210349, 'bagging_freq': 4}. Best is trial 0 with value: 0.4538571514173923.\n[I 2025-12-01 06:10:04,669] Trial 1 finished with value: 0.708227870863324 and parameters: {'num_leaves': 244, 'learning_rate': 0.014966181126900796, 'n_estimators': 917, 'feature_fraction': 0.7163606015558529, 'bagging_fraction': 0.7235413292205145, 'bagging_freq': 5}. Best is trial 0 with value: 0.4538571514173923.\n[I 2025-12-01 06:12:14,078] Trial 2 finished with value: 0.4817336330542664 and parameters: {'num_leaves': 375, 'learning_rate': 0.04736546661103953, 'n_estimators': 941, 'feature_fraction': 0.91415826942717, 'bagging_fraction': 0.8851016611103248, 'bagging_freq': 1}. Best is trial 0 with value: 0.4538571514173923.\n[I 2025-12-01 06:15:49,932] Trial 3 finished with value: 0.4376129610230302 and parameters: {'num_leaves': 386, 'learning_rate': 0.04801952650001557, 'n_estimators': 1588, 'feature_fraction': 0.7687325978629282, 'bagging_fraction': 0.7709936598863217, 'bagging_freq': 5}. Best is trial 3 with value: 0.4376129610230302.\n[I 2025-12-01 06:17:17,035] Trial 4 finished with value: 0.6560873221188771 and parameters: {'num_leaves': 223, 'learning_rate': 0.02436521166298609, 'n_estimators': 830, 'feature_fraction': 0.7864998457083076, 'bagging_fraction': 0.8754639638754295, 'bagging_freq': 1}. Best is trial 3 with value: 0.4376129610230302.\n[I 2025-12-01 06:20:33,249] Trial 5 finished with value: 0.463110145085284 and parameters: {'num_leaves': 331, 'learning_rate': 0.12196029902933608, 'n_estimators': 1543, 'feature_fraction': 0.9030738460936376, 'bagging_fraction': 0.7419922963315664, 'bagging_freq': 4}. Best is trial 3 with value: 0.4376129610230302.\n[I 2025-12-01 06:24:17,061] Trial 6 finished with value: 0.4441941917163006 and parameters: {'num_leaves': 450, 'learning_rate': 0.03644868022185584, 'n_estimators': 1439, 'feature_fraction': 0.7836709491552261, 'bagging_fraction': 0.8022142538300236, 'bagging_freq': 3}. Best is trial 3 with value: 0.4376129610230302.\n[I 2025-12-01 06:25:00,144] Trial 7 finished with value: 0.6761518930655596 and parameters: {'num_leaves': 84, 'learning_rate': 0.08196078935862629, 'n_estimators': 758, 'feature_fraction': 0.7974735132168761, 'bagging_fraction': 0.7199175532849625, 'bagging_freq': 1}. Best is trial 3 with value: 0.4376129610230302.\n[I 2025-12-01 06:27:48,812] Trial 8 finished with value: 0.44296217055895215 and parameters: {'num_leaves': 418, 'learning_rate': 0.0887361980875194, 'n_estimators': 1146, 'feature_fraction': 0.8351426700613221, 'bagging_fraction': 0.9027638569054391, 'bagging_freq': 4}. Best is trial 3 with value: 0.4376129610230302.\n[I 2025-12-01 06:29:02,290] Trial 9 finished with value: 0.7710900289311006 and parameters: {'num_leaves': 203, 'learning_rate': 0.019452824976592866, 'n_estimators': 619, 'feature_fraction': 0.7391548103920054, 'bagging_fraction': 0.8708199025779862, 'bagging_freq': 4}. Best is trial 3 with value: 0.4376129610230302.\n[I 2025-12-01 06:33:08,148] Trial 10 finished with value: 0.44839697837176906 and parameters: {'num_leaves': 502, 'learning_rate': 0.04786812302634354, 'n_estimators': 1251, 'feature_fraction': 0.995033635000866, 'bagging_fraction': 0.8018915641851142, 'bagging_freq': 5}. Best is trial 3 with value: 0.4376129610230302.\n[I 2025-12-01 06:36:11,172] Trial 11 finished with value: 0.43669476869144996 and parameters: {'num_leaves': 407, 'learning_rate': 0.06495143444778083, 'n_estimators': 1209, 'feature_fraction': 0.8386298735936714, 'bagging_fraction': 0.9571285932670991, 'bagging_freq': 3}. Best is trial 11 with value: 0.43669476869144996.\n[I 2025-12-01 06:39:33,840] Trial 12 finished with value: 0.48126803242505833 and parameters: {'num_leaves': 350, 'learning_rate': 0.03240304320240018, 'n_estimators': 1343, 'feature_fraction': 0.8414466879093908, 'bagging_fraction': 0.9963772357051093, 'bagging_freq': 2}. Best is trial 11 with value: 0.43669476869144996.\n[I 2025-12-01 06:42:49,589] Trial 13 finished with value: 0.44715243625526685 and parameters: {'num_leaves': 507, 'learning_rate': 0.05212857005331391, 'n_estimators': 1094, 'feature_fraction': 0.753664564323403, 'bagging_fraction': 0.8020558425572454, 'bagging_freq': 3}. Best is trial 11 with value: 0.43669476869144996.\n[I 2025-12-01 06:45:52,572] Trial 14 finished with value: 0.42964437062493316 and parameters: {'num_leaves': 423, 'learning_rate': 0.06362409917122343, 'n_estimators': 1338, 'feature_fraction': 0.700639025883399, 'bagging_fraction': 0.9330264942080349, 'bagging_freq': 3}. Best is trial 14 with value: 0.42964437062493316.\n[I 2025-12-01 06:48:34,440] Trial 15 finished with value: 0.4665016686796871 and parameters: {'num_leaves': 293, 'learning_rate': 0.1376791002790115, 'n_estimators': 1270, 'feature_fraction': 0.9617777229119043, 'bagging_fraction': 0.9336608927042789, 'bagging_freq': 3}. Best is trial 14 with value: 0.42964437062493316.\n[I 2025-12-01 06:51:20,408] Trial 16 finished with value: 0.4363637059993664 and parameters: {'num_leaves': 448, 'learning_rate': 0.06553200651010878, 'n_estimators': 1090, 'feature_fraction': 0.7182153860643595, 'bagging_fraction': 0.9355014588422909, 'bagging_freq': 2}. Best is trial 14 with value: 0.42964437062493316.\n[I 2025-12-01 06:53:59,184] Trial 17 finished with value: 0.43975920068710095 and parameters: {'num_leaves': 461, 'learning_rate': 0.0636280113975395, 'n_estimators': 1038, 'feature_fraction': 0.7015458319527712, 'bagging_fraction': 0.9247903924607314, 'bagging_freq': 2}. Best is trial 14 with value: 0.42964437062493316.\n[I 2025-12-01 06:56:25,846] Trial 18 finished with value: 0.7750367696883568 and parameters: {'num_leaves': 152, 'learning_rate': 0.011471827972944618, 'n_estimators': 1376, 'feature_fraction': 0.7327459175262943, 'bagging_fraction': 0.9992173735472063, 'bagging_freq': 2}. Best is trial 14 with value: 0.42964437062493316.\n[I 2025-12-01 06:58:18,958] Trial 19 finished with value: 0.4666390924542085 and parameters: {'num_leaves': 303, 'learning_rate': 0.11084443104053453, 'n_estimators': 995, 'feature_fraction': 0.704312188215822, 'bagging_fraction': 0.9590954430895628, 'bagging_freq': 2}. Best is trial 14 with value: 0.42964437062493316.\n[I 2025-12-01 07:01:28,339] Trial 20 finished with value: 0.48476869086103286 and parameters: {'num_leaves': 451, 'learning_rate': 0.02854383879608586, 'n_estimators': 1149, 'feature_fraction': 0.7388215647193757, 'bagging_fraction': 0.8526325038080992, 'bagging_freq': 2}. Best is trial 14 with value: 0.42964437062493316.\n[I 2025-12-01 07:04:32,590] Trial 21 finished with value: 0.4304622039815351 and parameters: {'num_leaves': 413, 'learning_rate': 0.06948119184079903, 'n_estimators': 1230, 'feature_fraction': 0.8158023954395449, 'bagging_fraction': 0.9560207353581017, 'bagging_freq': 3}. Best is trial 14 with value: 0.42964437062493316.\n[I 2025-12-01 07:07:53,434] Trial 22 finished with value: 0.42964326889627863 and parameters: {'num_leaves': 426, 'learning_rate': 0.06823805346215364, 'n_estimators': 1320, 'feature_fraction': 0.8127497210576685, 'bagging_fraction': 0.9162512972214516, 'bagging_freq': 3}. Best is trial 22 with value: 0.42964326889627863.\n[I 2025-12-01 07:10:46,540] Trial 23 finished with value: 0.44295571755530594 and parameters: {'num_leaves': 354, 'learning_rate': 0.10090833800415329, 'n_estimators': 1336, 'feature_fraction': 0.8065232352356576, 'bagging_fraction': 0.9100830034290869, 'bagging_freq': 3}. Best is trial 22 with value: 0.42964326889627863.\n[I 2025-12-01 07:14:26,116] Trial 24 finished with value: 0.42435911431036544 and parameters: {'num_leaves': 426, 'learning_rate': 0.07297348762176706, 'n_estimators': 1445, 'feature_fraction': 0.8145634343144817, 'bagging_fraction': 0.9747597911761802, 'bagging_freq': 3}. Best is trial 24 with value: 0.42435911431036544.\n[I 2025-12-01 07:18:36,771] Trial 25 finished with value: 0.42026061485092614 and parameters: {'num_leaves': 482, 'learning_rate': 0.05678346611026921, 'n_estimators': 1427, 'feature_fraction': 0.8581472296747062, 'bagging_fraction': 0.9811748487410205, 'bagging_freq': 4}. Best is trial 25 with value: 0.42026061485092614.\n[I 2025-12-01 07:22:55,424] Trial 26 finished with value: 0.42770116260016344 and parameters: {'num_leaves': 493, 'learning_rate': 0.04260556218916344, 'n_estimators': 1446, 'feature_fraction': 0.8642974448686918, 'bagging_fraction': 0.980013589026662, 'bagging_freq': 4}. Best is trial 25 with value: 0.42026061485092614.\n[I 2025-12-01 07:27:15,527] Trial 27 finished with value: 0.42980037803389004 and parameters: {'num_leaves': 486, 'learning_rate': 0.03914339024360584, 'n_estimators': 1441, 'feature_fraction': 0.8706771865526934, 'bagging_fraction': 0.9804170316683605, 'bagging_freq': 4}. Best is trial 25 with value: 0.42026061485092614.\n[I 2025-12-01 07:31:51,694] Trial 28 finished with value: 0.47239432683004173 and parameters: {'num_leaves': 473, 'learning_rate': 0.023421576246489054, 'n_estimators': 1440, 'feature_fraction': 0.8752668837848266, 'bagging_fraction': 0.9835880157566212, 'bagging_freq': 4}. Best is trial 25 with value: 0.42026061485092614.\n[I 2025-12-01 07:36:35,613] Trial 29 finished with value: 0.4313612540544281 and parameters: {'num_leaves': 485, 'learning_rate': 0.03832751314147389, 'n_estimators': 1499, 'feature_fraction': 0.906207215145591, 'bagging_fraction': 0.9745662576243744, 'bagging_freq': 4}. Best is trial 25 with value: 0.42026061485092614.\n  Best LGBM params (Bayes): {'num_leaves': 482, 'learning_rate': 0.05678346611026921, 'n_estimators': 1427, 'feature_fraction': 0.8581472296747062, 'bagging_fraction': 0.9811748487410205, 'bagging_freq': 4}, val RMSE: 0.4203\n\n[2.6] Bayesian hyperparameter tuning for XGBoost...\n  Subsampled train_tree: 1125872 -> 300000 rows for tuning\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-12-01 07:36:36,485] A new study created in memory with name: no-name-644151b1-11d6-4614-a1a0-54f09035085f\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ec97e1057294512953c9cd53cea6580"}},"metadata":{}},{"name":"stdout","text":"[I 2025-12-01 07:42:36,580] Trial 0 finished with value: 0.5952437249473718 and parameters: {'max_depth': 12, 'learning_rate': 0.013251541619361411, 'n_estimators': 704, 'subsample': 0.9922539421047041, 'colsample_bytree': 0.833717164705204}. Best is trial 0 with value: 0.5952437249473718.\n[I 2025-12-01 07:42:55,677] Trial 1 finished with value: 1.0590471878521315 and parameters: {'max_depth': 5, 'learning_rate': 0.09280077967432111, 'n_estimators': 411, 'subsample': 0.9346542133975306, 'colsample_bytree': 0.8705320791659639}. Best is trial 0 with value: 0.5952437249473718.\n[I 2025-12-01 07:44:50,563] Trial 2 finished with value: 0.7304613087376086 and parameters: {'max_depth': 10, 'learning_rate': 0.019618819358506387, 'n_estimators': 493, 'subsample': 0.9993726551503224, 'colsample_bytree': 0.9866465516210872}. Best is trial 0 with value: 0.5952437249473718.\n[I 2025-12-01 07:45:26,010] Trial 3 finished with value: 0.7945003722739794 and parameters: {'max_depth': 6, 'learning_rate': 0.1180650977706101, 'n_estimators': 715, 'subsample': 0.9966040969974814, 'colsample_bytree': 0.706210920965987}. Best is trial 0 with value: 0.5952437249473718.\n[I 2025-12-01 07:48:46,095] Trial 4 finished with value: 0.7025004804135535 and parameters: {'max_depth': 11, 'learning_rate': 0.01174377633675121, 'n_estimators': 559, 'subsample': 0.9112249192201713, 'colsample_bytree': 0.8001011824037174}. Best is trial 0 with value: 0.5952437249473718.\n[I 2025-12-01 07:52:05,259] Trial 5 finished with value: 0.47093899565197767 and parameters: {'max_depth': 12, 'learning_rate': 0.0720848968282149, 'n_estimators': 494, 'subsample': 0.783035368688163, 'colsample_bytree': 0.7461517910659078}. Best is trial 5 with value: 0.47093899565197767.\n[I 2025-12-01 07:53:08,423] Trial 6 finished with value: 0.7679556429284499 and parameters: {'max_depth': 7, 'learning_rate': 0.0476639359272616, 'n_estimators': 891, 'subsample': 0.9887006831092588, 'colsample_bytree': 0.9948718500673743}. Best is trial 5 with value: 0.47093899565197767.\n[I 2025-12-01 07:54:18,527] Trial 7 finished with value: 0.7738378315614185 and parameters: {'max_depth': 9, 'learning_rate': 0.02436438848972852, 'n_estimators': 489, 'subsample': 0.9837319503276035, 'colsample_bytree': 0.9247910547098341}. Best is trial 5 with value: 0.47093899565197767.\n[I 2025-12-01 07:59:58,571] Trial 8 finished with value: 0.6070689330167736 and parameters: {'max_depth': 12, 'learning_rate': 0.012295139991824418, 'n_estimators': 712, 'subsample': 0.7166932077470989, 'colsample_bytree': 0.8055546416780365}. Best is trial 5 with value: 0.47093899565197767.\n[I 2025-12-01 08:00:47,321] Trial 9 finished with value: 0.624930475723918 and parameters: {'max_depth': 8, 'learning_rate': 0.11919184378474644, 'n_estimators': 530, 'subsample': 0.7730931248512753, 'colsample_bytree': 0.7954351671386131}. Best is trial 5 with value: 0.47093899565197767.\n[I 2025-12-01 08:02:38,026] Trial 10 finished with value: 0.5139898035264109 and parameters: {'max_depth': 10, 'learning_rate': 0.06463326460786133, 'n_estimators': 624, 'subsample': 0.81859653514196, 'colsample_bytree': 0.7021440744195968}. Best is trial 5 with value: 0.47093899565197767.\n[I 2025-12-01 08:04:24,706] Trial 11 finished with value: 0.5262501692677541 and parameters: {'max_depth': 10, 'learning_rate': 0.05853629572591412, 'n_estimators': 611, 'subsample': 0.8149580578317208, 'colsample_bytree': 0.7023475540289285}. Best is trial 5 with value: 0.47093899565197767.\n[I 2025-12-01 08:06:16,317] Trial 12 finished with value: 0.5113048974926251 and parameters: {'max_depth': 11, 'learning_rate': 0.0696547031335927, 'n_estimators': 412, 'subsample': 0.840288082728563, 'colsample_bytree': 0.7399762382736397}. Best is trial 5 with value: 0.47093899565197767.\n[I 2025-12-01 08:09:17,638] Trial 13 finished with value: 0.5467175169724251 and parameters: {'max_depth': 12, 'learning_rate': 0.03264676076403923, 'n_estimators': 413, 'subsample': 0.8657085998088985, 'colsample_bytree': 0.75439341689124}. Best is trial 5 with value: 0.47093899565197767.\n[I 2025-12-01 08:11:20,531] Trial 14 finished with value: 0.5013244784608376 and parameters: {'max_depth': 11, 'learning_rate': 0.07674363769526633, 'n_estimators': 448, 'subsample': 0.7476371197088705, 'colsample_bytree': 0.7544490672351539}. Best is trial 5 with value: 0.47093899565197767.\n[I 2025-12-01 08:13:29,617] Trial 15 finished with value: 0.49465655164680405 and parameters: {'max_depth': 11, 'learning_rate': 0.08846496086623398, 'n_estimators': 474, 'subsample': 0.7330019843248474, 'colsample_bytree': 0.7584622982244565}. Best is trial 5 with value: 0.47093899565197767.\n[I 2025-12-01 08:14:46,555] Trial 16 finished with value: 0.6573823604003033 and parameters: {'max_depth': 9, 'learning_rate': 0.04084026232461545, 'n_estimators': 560, 'subsample': 0.7759547552573994, 'colsample_bytree': 0.900911439174839}. Best is trial 5 with value: 0.47093899565197767.\n[I 2025-12-01 08:18:46,500] Trial 17 finished with value: 0.4643409659750156 and parameters: {'max_depth': 11, 'learning_rate': 0.09798131909611066, 'n_estimators': 867, 'subsample': 0.7031182190244448, 'colsample_bytree': 0.7759353774478893}. Best is trial 17 with value: 0.4643409659750156.\n[I 2025-12-01 08:20:10,407] Trial 18 finished with value: 0.5607544857294114 and parameters: {'max_depth': 8, 'learning_rate': 0.13820570591347198, 'n_estimators': 899, 'subsample': 0.7081634906763238, 'colsample_bytree': 0.8452003554589788}. Best is trial 17 with value: 0.4643409659750156.\n[I 2025-12-01 08:25:48,711] Trial 19 finished with value: 0.4492939987778791 and parameters: {'max_depth': 12, 'learning_rate': 0.047793825062604524, 'n_estimators': 822, 'subsample': 0.7708991915948531, 'colsample_bytree': 0.7815535435182758}. Best is trial 19 with value: 0.4492939987778791.\n[I 2025-12-01 08:28:18,864] Trial 20 finished with value: 0.5575154086606657 and parameters: {'max_depth': 10, 'learning_rate': 0.030970342829386318, 'n_estimators': 815, 'subsample': 0.7521730478933609, 'colsample_bytree': 0.7886695370182844}. Best is trial 19 with value: 0.4492939987778791.\n[I 2025-12-01 08:33:36,725] Trial 21 finished with value: 0.44696011697115506 and parameters: {'max_depth': 12, 'learning_rate': 0.05094436734190166, 'n_estimators': 812, 'subsample': 0.790928386551611, 'colsample_bytree': 0.728935573960325}. Best is trial 21 with value: 0.44696011697115506.\n[I 2025-12-01 08:39:08,375] Trial 22 finished with value: 0.4495096534358835 and parameters: {'max_depth': 12, 'learning_rate': 0.048159777383233085, 'n_estimators': 815, 'subsample': 0.8018798077442101, 'colsample_bytree': 0.7797008122291431}. Best is trial 21 with value: 0.44696011697115506.\n[I 2025-12-01 08:44:39,048] Trial 23 finished with value: 0.4494879451986704 and parameters: {'max_depth': 12, 'learning_rate': 0.05111709859693186, 'n_estimators': 793, 'subsample': 0.8024315452933177, 'colsample_bytree': 0.8184851833778948}. Best is trial 21 with value: 0.44696011697115506.\n[I 2025-12-01 08:50:00,596] Trial 24 finished with value: 0.4526504865325934 and parameters: {'max_depth': 12, 'learning_rate': 0.04900656384952307, 'n_estimators': 765, 'subsample': 0.8552842263430375, 'colsample_bytree': 0.824027234178015}. Best is trial 21 with value: 0.44696011697115506.\n  Best XGB params (Bayes): {'max_depth': 12, 'learning_rate': 0.05094436734190166, 'n_estimators': 812, 'subsample': 0.790928386551611, 'colsample_bytree': 0.728935573960325}, val RMSE: 0.4470\n\n  Training LGBM ensemble with tuned params: {'objective': 'regression', 'boosting_type': 'gbdt', 'min_data_in_leaf': 50, 'feature_fraction': 0.8581472296747062, 'bagging_fraction': 0.9811748487410205, 'bagging_freq': 4, 'n_estimators': 1427, 'learning_rate': 0.05678346611026921, 'num_leaves': 482, 'verbosity': -1}\n   -> LGBM model 1/3 for dx...\n   -> LGBM model 1/3 for dy...\n   -> LGBM model 2/3 for dx...\n   -> LGBM model 2/3 for dy...\n   -> LGBM model 3/3 for dx...\n   -> LGBM model 3/3 for dy...\n✓ LGBM ensemble trained on full dataset\n\n  Training XGBoost models with tuned params: {'objective': 'reg:squarederror', 'tree_method': 'hist', 'enable_categorical': True, 'n_jobs': -1, 'reg_lambda': 1.0, 'random_state': 42, 'max_depth': 12, 'learning_rate': 0.05094436734190166, 'n_estimators': 812, 'subsample': 0.790928386551611, 'colsample_bytree': 0.728935573960325}\n   -> XGB model for dx...\n   -> XGB model for dy...\n✓ XGBoost models trained on full dataset\n  TRAIN_GNN=False -> skipping GNN training\n\n[4/4] Tuning ensemble weights (LGBM / XGB / GNN)...\n  Subsampled train_tree: 1125872 -> 50000 rows for ensemble tuning\n  Single-model RMSE [LGBM] = 0.1546\n  Single-model RMSE [XGB ] = 0.1456\n  Single-model RMSE [GNN ] = 4.2914\n  Searching weights for 3-model ensemble (LGBM+XGB+GNN) with step=0.1 ...\n  Best ensemble weights: LGBM=0.40, XGB=0.60, GNN=-0.00, RMSE=0.1345\n  Saved 3 LGBM dx and 3 LGBM dy models to models\n  Saved 1 XGB dx and 1 XGB dy models to models\n  Saved metadata to meta.pkl\n\nTraining finished. Models + meta saved in ./models\n","output_type":"stream"}],"execution_count":1}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":114239,"databundleVersionId":14210809,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":13934348,"sourceType":"datasetVersion","datasetId":8880099},{"sourceId":13936209,"sourceType":"datasetVersion","datasetId":8881449}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# %% [markdown]\n# # NFL BDB 2026 – LGBM Inference-Only Submission (Using Saved Models)\n#\n# This notebook:\n# - Loads pretrained LightGBM models from a `models.zip` dataset\n# - Reuses the same feature engineering as the training script\n# - Implements `predict(test, test_input)` for the Kaggle evaluation server\n# - Optionally runs the local gateway to create `submission.csv` for debugging\n#\n# IMPORTANT:\n# - Make sure you've attached **two** datasets in the \"Data\" tab:\n#   1. `nfl-big-data-bowl-2026-prediction`  (competition data)\n#   2. Your `models.zip` dataset with the saved LGBM models\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n# ======================== IMPORTS & CONFIG ========================\nimport os\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom lightgbm import LGBMRegressor\nimport xgboost as xgb\nimport joblib\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# ================== POST-PROCESSING CONSTANTS ==================\nMAX_STEP_ABS = 20.0     # max absolute offset from x_last in yards (per frame)\nSMOOTHING_ALPHA = 0.8   # EMA: current frame weight; (1-alpha) for previous frame\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-30T17:03:42.277974Z","iopub.execute_input":"2025-11-30T17:03:42.278319Z","iopub.status.idle":"2025-11-30T17:03:53.564969Z","shell.execute_reply.started":"2025-11-30T17:03:42.278293Z","shell.execute_reply":"2025-11-30T17:03:53.563724Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Path to competition data\nimport sys\nDATA_DIR = \"/kaggle/input/nfl-big-data-bowl-2026-prediction\"\n\n# Path to your models (as shown in the screenshot)\n# dataset slug: \"models\"\n# folder inside: \"models\"\nMODELS_DIR = \"/kaggle/input/models2/models\"\n\n# Make Kaggle evaluation module importable\nsys.path.append(DATA_DIR)\ntry:\n    from kaggle_evaluation.nfl_inference_server import NFLInferenceServer\n    HAS_EVAL_SERVER = True\nexcept ModuleNotFoundError:\n    NFLInferenceServer = None\n    HAS_EVAL_SERVER = False\n    print(\n        \"WARNING: kaggle_evaluation not found. \"\n        \"If you're running locally, this is expected.\"\n    )\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\ntorch.manual_seed(RANDOM_STATE)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(\"=\" * 70)\nprint(\"NFL BIG DATA BOWL 2026 - LGBM Inference-Only (Saved Models)\")\nprint(\"=\" * 70)\nprint(\"DEVICE:\", DEVICE)\nprint(\"MODELS_DIR:\", MODELS_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T17:03:53.566850Z","iopub.execute_input":"2025-11-30T17:03:53.567608Z","iopub.status.idle":"2025-11-30T17:03:53.584005Z","shell.execute_reply.started":"2025-11-30T17:03:53.567577Z","shell.execute_reply":"2025-11-30T17:03:53.582904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code]\n# ======================== FEATURE LISTS ===========================\nFEATURES = [\n    \"x_last\", \"y_last\",\n    \"s\", \"a\", \"o\", \"dir\",\n    \"vx\", \"vy\",\n    \"ax_comp\", \"ay_comp\",\n    \"dir_sin\", \"dir_cos\",\n    \"o_sin\", \"o_cos\",\n    \"frame_offset\", \"time_offset\",\n    \"num_frames_output\",\n    \"frac_of_flight\",\n    \"frames_left\",\n    \"time_to_land\",\n    \"remaining_flight_frac\",\n    \"dist_to_ball_land\",\n    \"angle_to_ball_land\",\n    \"dist_to_ball_land_per_frame\",\n    \"cos_dir_to_ball\",\n    \"cos_orient_to_ball\",\n    \"x_rel_ball\",\n    \"y_rel_ball\",\n    \"v_toward_ball\",\n    \"v_across_ball\",\n    \"x_std\",\n    \"ball_land_x_std\",\n    \"dx_to_land_std\",\n    \"dy_to_land\",\n    \"dist_to_sideline\",\n    \"dist_to_center\",\n    \"yardline_100\",\n    \"yardline_norm\",\n    \"dist_to_endzone\",\n    \"dist_to_target_last\",\n    \"dx_to_target_last\",\n    \"dy_to_target_last\",\n    \"angle_to_target\",\n    \"cos_dir_to_target\",\n    \"cos_orient_to_target\",\n    \"v_toward_target\",\n    \"v_across_target\",\n    \"is_target\",\n    \"absolute_yardline_number\",\n    \"player_height\", \"player_weight\",\n    \"bmi\",\n    \"min_dist_teammate\",\n    \"mean_dist_teammate\",\n    \"min_dist_opponent\",\n    \"mean_dist_opponent\",\n]\n\nCAT_FEATS = [\"player_role\", \"player_side\", \"play_direction\"]\n\nBASE_COLS = [\n    \"game_id\", \"play_id\", \"nfl_id\",\n    \"x_last\", \"y_last\",\n    \"s\", \"a\", \"o\", \"dir\",\n    \"player_role\", \"player_side\",\n    \"num_frames_output\",\n    \"ball_land_x\", \"ball_land_y\",\n    \"target_last_x\", \"target_last_y\", \"target_nfl_id\",\n    \"play_direction\",\n    \"absolute_yardline_number\",\n    \"player_height\", \"player_weight\",\n    \"player_to_predict\",\n    \"min_dist_teammate\",\n    \"mean_dist_teammate\",\n    \"min_dist_opponent\",\n    \"mean_dist_opponent\",\n]\n\n# Global LGBM models (will be filled by loader)\nLGBM_MODELS_DX = []\nLGBM_MODELS_DY = []\n\n\n# ======================== FEATURE HELPERS =========================\ndef height_to_inches(ht):\n    \"\"\"Convert '6-2' -> inches.\"\"\"\n    if isinstance(ht, str) and \"-\" in ht:\n        try:\n            feet, inches = ht.split(\"-\")\n            return int(feet) * 12 + int(inches)\n        except Exception:\n            return np.nan\n    return np.nan\n\n\ndef add_team_distance_features(df_last: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Pairwise distances within (game_id, play_id):\n    - min/mean distance to teammates\n    - min/mean distance to opponents\n    \"\"\"\n    if \"player_side\" not in df_last.columns:\n        df_last[\"min_dist_teammate\"] = 0.0\n        df_last[\"mean_dist_teammate\"] = 0.0\n        df_last[\"min_dist_opponent\"] = 0.0\n        df_last[\"mean_dist_opponent\"] = 0.0\n        return df_last\n\n    groups = []\n    for (_, _), g in df_last.groupby([\"game_id\", \"play_id\"], as_index=False):\n        g = g.copy()\n        xs = g[\"x_last\"].to_numpy()\n        ys = g[\"y_last\"].to_numpy()\n        sides = g[\"player_side\"].astype(\"category\").cat.codes.to_numpy()\n\n        dx = xs[:, None] - xs[None, :]\n        dy = ys[:, None] - ys[None, :]\n        dist = np.sqrt(dx * dx + dy * dy)\n        np.fill_diagonal(dist, np.inf)\n\n        same = sides[:, None] == sides[None, :]\n        opp = ~same\n\n        dist_tm = np.where(same, dist, np.inf)\n        min_dist_tm = dist_tm.min(axis=1)\n        min_dist_tm[np.isinf(min_dist_tm)] = 0.0\n\n        sum_tm = np.where(same, dist, 0.0).sum(axis=1)\n        cnt_tm = same.sum(axis=1) - 1\n        mean_tm = np.divide(\n            sum_tm,\n            np.maximum(cnt_tm, 1),\n            out=np.zeros_like(sum_tm),\n            where=cnt_tm > 0,\n        )\n\n        dist_op = np.where(opp, dist, np.inf)\n        min_dist_op = dist_op.min(axis=1)\n        min_dist_op[np.isinf(min_dist_op)] = 0.0\n\n        sum_op = np.where(opp, dist, 0.0).sum(axis=1)\n        cnt_op = opp.sum(axis=1)\n        mean_op = np.divide(\n            sum_op,\n            np.maximum(cnt_op, 1),\n            out=np.zeros_like(sum_op),\n            where=cnt_op > 0,\n        )\n\n        g[\"min_dist_teammate\"] = min_dist_tm\n        g[\"mean_dist_teammate\"] = mean_tm\n        g[\"min_dist_opponent\"] = min_dist_op\n        g[\"mean_dist_opponent\"] = mean_op\n\n        groups.append(g)\n\n    return pd.concat(groups, ignore_index=True)\n\n\ndef prepare_last_obs(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Last observation per (game_id, play_id, nfl_id),\n    rename x,y -> x_last, y_last, convert height,\n    add team distance features.\n    \"\"\"\n    df_last = (\n        df.sort_values([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n          .groupby([\"game_id\", \"play_id\", \"nfl_id\"], as_index=False)\n          .last()\n    )\n    df_last = df_last.rename(columns={\"x\": \"x_last\", \"y\": \"y_last\"})\n\n    if \"player_height\" in df_last.columns:\n        df_last[\"player_height\"] = df_last[\"player_height\"].apply(height_to_inches)\n    else:\n        df_last[\"player_height\"] = np.nan\n\n    df_last = add_team_distance_features(df_last)\n    return df_last\n\n\ndef add_target_info(df_last: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Attach targeted receiver coords to every row in the play.\n    \"\"\"\n    mask_target = df_last.get(\"player_role\", \"\") == \"Targeted Receiver\"\n    targets = df_last.loc[\n        mask_target,\n        [\"game_id\", \"play_id\", \"nfl_id\", \"x_last\", \"y_last\"],\n    ].copy()\n\n    targets = targets.rename(\n        columns={\n            \"nfl_id\": \"target_nfl_id\",\n            \"x_last\": \"target_last_x\",\n            \"y_last\": \"target_last_y\",\n        }\n    )\n\n    df_last = df_last.merge(\n        targets[[\"game_id\", \"play_id\", \"target_last_x\", \"target_last_y\", \"target_nfl_id\"]],\n        on=[\"game_id\", \"play_id\"],\n        how=\"left\",\n    )\n    return df_last\n\n\ndef mirror_raw(df_raw: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Mirror across field width: y -> 53.3 - y, adjust angles.\n    (Not needed in inference; used in training for augmentation.)\n    \"\"\"\n    df = df_raw.copy()\n\n    if \"y_last\" in df.columns:\n        df[\"y_last\"] = 53.3 - df[\"y_last\"]\n    if \"y\" in df.columns:\n        df[\"y\"] = 53.3 - df[\"y\"]\n    if \"ball_land_y\" in df.columns:\n        df[\"ball_land_y\"] = 53.3 - df[\"ball_land_y\"]\n    if \"target_last_y\" in df.columns:\n        df[\"target_last_y\"] = 53.3 - df[\"target_last_y\"]\n\n    for ang_col in [\"dir\", \"o\"]:\n        if ang_col in df.columns:\n            df[ang_col] = (-df[ang_col]) % 360.0\n\n    return df\n\n\ndef create_features(df: pd.DataFrame, is_train: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Full feature engineering – identical to training script.\n    \"\"\"\n    df = df.copy()\n\n    s = df[\"s\"].fillna(0.0)\n    a = df[\"a\"].fillna(0.0)\n    dir_rad = np.deg2rad(df[\"dir\"].fillna(0.0))\n    o_rad = np.deg2rad(df[\"o\"].fillna(0.0))\n\n    df[\"vx\"] = s * np.cos(dir_rad)\n    df[\"vy\"] = s * np.sin(dir_rad)\n    df[\"ax_comp\"] = a * np.cos(dir_rad)\n    df[\"ay_comp\"] = a * np.sin(dir_rad)\n\n    df[\"dir_sin\"] = np.sin(dir_rad)\n    df[\"dir_cos\"] = np.cos(dir_rad)\n    df[\"o_sin\"] = np.sin(o_rad)\n    df[\"o_cos\"] = np.cos(o_rad)\n\n    # frame / time\n    if \"frame_id\" in df.columns:\n        df[\"frame_offset\"] = df[\"frame_id\"]\n    else:\n        df[\"frame_offset\"] = 0\n\n    df[\"time_offset\"] = df[\"frame_offset\"] / 10.0\n\n    if \"num_frames_output\" in df.columns:\n        nfo = df[\"num_frames_output\"].replace(0, np.nan)\n        df[\"frac_of_flight\"] = (df[\"frame_offset\"] / nfo).clip(lower=0, upper=1)\n        df[\"frac_of_flight\"] = df[\"frac_of_flight\"].fillna(0.0)\n        df[\"frames_left\"] = (nfo - df[\"frame_offset\"]).clip(lower=0).fillna(0.0)\n    else:\n        df[\"frac_of_flight\"] = 0.0\n        df[\"frames_left\"] = 0.0\n\n    df[\"time_to_land\"] = df[\"frames_left\"] / 10.0\n    df[\"remaining_flight_frac\"] = (1.0 - df[\"frac_of_flight\"]).clip(lower=0.0, upper=1.0)\n\n    # ball landing geometry\n    df[\"dist_to_ball_land\"] = np.sqrt(\n        (df[\"ball_land_x\"] - df[\"x_last\"]) ** 2 +\n        (df[\"ball_land_y\"] - df[\"y_last\"]) ** 2\n    )\n    df[\"angle_to_ball_land\"] = np.arctan2(\n        df[\"ball_land_y\"] - df[\"y_last\"],\n        df[\"ball_land_x\"] - df[\"x_last\"],\n    )\n\n    frames_left_safe = df[\"frames_left\"].replace(0, np.nan)\n    df[\"dist_to_ball_land_per_frame\"] = df[\"dist_to_ball_land\"] / frames_left_safe\n    df[\"dist_to_ball_land_per_frame\"] = (\n        df[\"dist_to_ball_land_per_frame\"]\n        .replace([np.inf, -np.inf], np.nan)\n        .fillna(0.0)\n    )\n\n    df[\"cos_dir_to_ball\"] = np.cos(df[\"angle_to_ball_land\"] - dir_rad)\n    df[\"cos_orient_to_ball\"] = np.cos(df[\"angle_to_ball_land\"] - o_rad)\n\n    # direction-standardized coords\n    play_dir = df.get(\"play_direction\", \"right\").fillna(\"right\")\n    is_left = (play_dir == \"left\").astype(int)\n\n    df[\"x_std\"] = np.where(is_left == 1, 120.0 - df[\"x_last\"], df[\"x_last\"])\n    df[\"ball_land_x_std\"] = np.where(\n        is_left == 1, 120.0 - df[\"ball_land_x\"], df[\"ball_land_x\"]\n    )\n\n    df[\"dx_to_land_std\"] = df[\"ball_land_x_std\"] - df[\"x_std\"]\n    df[\"dy_to_land\"] = df[\"ball_land_y\"] - df[\"y_last\"]\n\n    # field position\n    df[\"dist_to_sideline\"] = np.minimum(df[\"y_last\"], 53.3 - df[\"y_last\"])\n    df[\"dist_to_center\"] = np.abs(df[\"y_last\"] - 53.3 / 2.0)\n\n    yard = df[\"absolute_yardline_number\"].fillna(50.0)\n    yard_100 = yard.clip(lower=0.0, upper=100.0)\n    df[\"yardline_100\"] = yard_100\n    df[\"yardline_norm\"] = yard_100 / 100.0\n    df[\"dist_to_endzone\"] = 100.0 - yard_100\n\n    # target receiver geometry\n    df[\"dist_to_target_last\"] = np.sqrt(\n        (df[\"target_last_x\"] - df[\"x_last\"]) ** 2 +\n        (df[\"target_last_y\"] - df[\"y_last\"]) ** 2\n    )\n    df[\"dx_to_target_last\"] = df[\"target_last_x\"] - df[\"x_last\"]\n    df[\"dy_to_target_last\"] = df[\"target_last_y\"] - df[\"y_last\"]\n    df[\"angle_to_target\"] = np.arctan2(\n        df[\"target_last_y\"] - df[\"y_last\"],\n        df[\"target_last_x\"] - df[\"x_last\"],\n    )\n\n    df[\"cos_dir_to_target\"] = np.cos(df[\"angle_to_target\"] - dir_rad)\n    df[\"cos_orient_to_target\"] = np.cos(df[\"angle_to_target\"] - o_rad)\n\n    df[\"is_target\"] = (df[\"nfl_id\"] == df[\"target_nfl_id\"]).astype(int)\n\n    # relative coords & velocity projections\n    df[\"x_rel_ball\"] = df[\"x_last\"] - df[\"ball_land_x\"]\n    df[\"y_rel_ball\"] = df[\"y_last\"] - df[\"ball_land_y\"]\n\n    vx = df[\"vx\"]\n    vy = df[\"vy\"]\n\n    ball_cos = np.cos(df[\"angle_to_ball_land\"])\n    ball_sin = np.sin(df[\"angle_to_ball_land\"])\n    df[\"v_toward_ball\"] = vx * ball_cos + vy * ball_sin\n    df[\"v_across_ball\"] = vx * (-ball_sin) + vy * ball_cos\n\n    tgt_cos = np.cos(df[\"angle_to_target\"])\n    tgt_sin = np.sin(df[\"angle_to_target\"])\n    df[\"v_toward_target\"] = vx * tgt_cos + vy * tgt_sin\n    df[\"v_across_target\"] = vx * (-tgt_sin) + vy * tgt_cos\n\n    df[[\"v_toward_ball\", \"v_across_ball\", \"v_toward_target\", \"v_across_target\"]] = (\n        df[[\"v_toward_ball\", \"v_across_ball\", \"v_toward_target\", \"v_across_target\"]]\n        .replace([np.inf, -np.inf], np.nan)\n        .fillna(0.0)\n    )\n\n    # player physics\n    h = df[\"player_height\"].replace(0, np.nan)\n    w = df[\"player_weight\"].replace(0, np.nan)\n    df[\"bmi\"] = 703.0 * (w / (h ** 2))\n    df[\"bmi\"] = df[\"bmi\"].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # targets only in train (for compatibility; in inference we don't use dx/dy)\n    if is_train:\n        df[\"dx\"] = df[\"x\"] - df[\"x_last\"]\n        df[\"dy\"] = df[\"y\"] - df[\"y_last\"]\n\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T17:04:28.324775Z","iopub.execute_input":"2025-11-30T17:04:28.325107Z","iopub.status.idle":"2025-11-30T17:04:28.364710Z","shell.execute_reply.started":"2025-11-30T17:04:28.325083Z","shell.execute_reply":"2025-11-30T17:04:28.363712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] cell 4\ndef prepare_inference_batch(test_pd: pd.DataFrame, test_input_pd: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Build rows for inference:\n    - last observation per (game_id, play_id, nfl_id) from test_input\n    - attach target receiver info and pairwise distances\n    - merge with test (id, game_id, play_id, nfl_id, frame_id)\n    - create features (same as training)\n    \"\"\"\n    last_obs = prepare_last_obs(test_input_pd)\n    last_obs = add_target_info(last_obs)\n\n    cols_to_keep_existing = [c for c in BASE_COLS if c in last_obs.columns]\n\n    test_rows = test_pd.merge(\n        last_obs[cols_to_keep_existing],\n        on=[\"game_id\", \"play_id\", \"nfl_id\"],\n        how=\"left\",\n    )\n\n    test_rows = create_features(test_rows, is_train=False)\n    return test_rows\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T17:04:55.400568Z","iopub.execute_input":"2025-11-30T17:04:55.401389Z","iopub.status.idle":"2025-11-30T17:04:55.407098Z","shell.execute_reply.started":"2025-11-30T17:04:55.401354Z","shell.execute_reply":"2025-11-30T17:04:55.406097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code]\n# ======================== LOAD SAVED LGBM MODELS =========================\nimport glob\nimport joblib\nfrom pathlib import Path\n\ndef load_lgbm_models():\n    \"\"\"\n    Load pretrained LGBM dx/dy models directly from MODELS_DIR.\n\n    Expected files (as in your dataset screenshot):\n\n      /kaggle/input/models/models/\n        lgbm_dx_0.pkl\n        lgbm_dx_1.pkl\n        lgbm_dy_0.pkl\n        lgbm_dy_1.pkl\n        lgbm_dy_2.pkl\n        (plus gnn_model.pth, xgb_dx_0.pkl, xgb_dy_0.pkl, meta.pkl)\n\n    Only the LGBM files are used for this submission.\n    \"\"\"\n    global LGBM_MODELS_DX, LGBM_MODELS_DY\n\n    models_path = Path(MODELS_DIR)\n    if not models_path.exists():\n        raise FileNotFoundError(\n            f\"{models_path} does not exist. \"\n            \"Check that the 'models' dataset is attached and the path is correct.\"\n        )\n\n    # Find dx/dy model files\n    dx_files = sorted(glob.glob(str(models_path / \"lgbm_dx_*.pkl\")))\n    dy_files = sorted(glob.glob(str(models_path / \"lgbm_dy_*.pkl\")))\n\n    if not dx_files or not dy_files:\n        raise FileNotFoundError(\n            f\"No LGBM dx/dy model files found in {models_path}.\\n\"\n            f\"Found: {[p.name for p in models_path.iterdir()]}\"\n        )\n\n    print(\"Found LGBM dx model files:\", dx_files)\n    print(\"Found LGBM dy model files:\", dy_files)\n\n    # Load all models for ensemble\n    LGBM_MODELS_DX = [joblib.load(f) for f in dx_files]\n    LGBM_MODELS_DY = [joblib.load(f) for f in dy_files]\n\n    print(\n        f\"Loaded {len(LGBM_MODELS_DX)} dx models and \"\n        f\"{len(LGBM_MODELS_DY)} dy models.\"\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict(test: pl.DataFrame, test_input: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Main predict() used by the Kaggle evaluation server.\n\n    - Uses saved LGBM models (dx, dy) to predict offsets from x_last, y_last\n    - Applies simple post-processing:\n        * clip dx, dy to avoid extreme jumps\n        * smooth x, y over frame_id within each (game_id, play_id, nfl_id)\n    - Returns Polars DataFrame with columns [\"x\", \"y\"] in the same row order as `test`.\n    \"\"\"\n    assert LGBM_MODELS_DX and LGBM_MODELS_DY, \"LGBM models are not loaded\"\n\n    # Convert to pandas for feature pipeline\n    test_pd = test.to_pandas()\n    test_input_pd = test_input.to_pandas()\n\n    # Build feature rows (same as in training)\n    test_rows = prepare_inference_batch(test_pd, test_input_pd)\n\n    # Ensure all features exist\n    for col in FEATURES:\n        if col not in test_rows.columns:\n            test_rows[col] = 0.0\n    for c in CAT_FEATS:\n        if c not in test_rows.columns:\n            test_rows[c] = \"unknown\"\n\n    # Categorical handling – same as training\n    X_tree = test_rows[FEATURES + CAT_FEATS].copy()\n    for c in CAT_FEATS:\n        X_tree[c] = X_tree[c].astype(\"category\")\n\n    # ---- LGBM ensemble predictions for dx, dy ----\n    pred_dx_tree_list = [m.predict(X_tree) for m in LGBM_MODELS_DX]\n    pred_dy_tree_list = [m.predict(X_tree) for m in LGBM_MODELS_DY]\n    dx_pred = np.mean(pred_dx_tree_list, axis=0)\n    dy_pred = np.mean(pred_dy_tree_list, axis=0)\n\n    # ---- 1) Clip dx, dy to avoid extreme offsets ----\n    dx_pred = np.clip(dx_pred, -MAX_STEP_ABS, MAX_STEP_ABS)\n    dy_pred = np.clip(dy_pred, -MAX_STEP_ABS, MAX_STEP_ABS)\n\n    # ---- 2) Convert to raw x, y (before smoothing) ----\n    x_raw = test_rows[\"x_last\"].to_numpy() + dx_pred\n    y_raw = test_rows[\"y_last\"].to_numpy() + dy_pred\n\n    # Clip to field bounds\n    x_raw = np.clip(x_raw, 0.0, 120.0)\n    y_raw = np.clip(y_raw, 0.0, 53.3)\n\n    # Put into a DataFrame with keys & frame_id for smoothing\n    out_df = pd.DataFrame({\n        \"game_id\": test_rows[\"game_id\"].values,\n        \"play_id\": test_rows[\"play_id\"].values,\n        \"nfl_id\":  test_rows[\"nfl_id\"].values,\n        \"frame_id\": test_rows.get(\"frame_id\", pd.Series(0, index=test_rows.index)).values,\n        \"x_raw\": x_raw,\n        \"y_raw\": y_raw,\n    })\n    out_df[\"row_idx\"] = np.arange(len(out_df))  # to restore original order later\n\n    # ---- 3) Smooth trajectories over time per (game, play, player) ----\n    # Sort by grouping keys + frame_id\n    out_df_sorted = out_df.sort_values(\n        [\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"row_idx\"]\n    ).reset_index(drop=True)\n\n    x_smooth = out_df_sorted[\"x_raw\"].to_numpy().copy()\n    y_smooth = out_df_sorted[\"y_raw\"].to_numpy().copy()\n\n    # EMA smoothing within each group\n    group_keys = [\"game_id\", \"play_id\", \"nfl_id\"]\n    group_offsets = (\n        out_df_sorted.groupby(group_keys, sort=False).indices.values()\n    )\n\n    # group_offsets is a dict-like, so we iterate properly\n    for idxs in out_df_sorted.groupby(group_keys, sort=False).indices.values():\n        # idxs is a numpy array of row indices for this group, already sorted by frame_id\n        if len(idxs) <= 1:\n            continue\n        for i in range(1, len(idxs)):\n            cur = idxs[i]\n            prev = idxs[i - 1]\n            x_smooth[cur] = (\n                SMOOTHING_ALPHA * x_smooth[cur] + (1.0 - SMOOTHING_ALPHA) * x_smooth[prev]\n            )\n            y_smooth[cur] = (\n                SMOOTHING_ALPHA * y_smooth[cur] + (1.0 - SMOOTHING_ALPHA) * y_smooth[prev]\n            )\n\n    # Attach smoothed coords\n    out_df_sorted[\"x_smooth\"] = x_smooth\n    out_df_sorted[\"y_smooth\"] = y_smooth\n\n    # Bring back to original row order using row_idx\n    out_final = (\n        out_df_sorted\n        .sort_values(\"row_idx\")\n        .reset_index(drop=True)\n    )\n\n    # Final x, y (smoothed, within bounds)\n    x_final = np.clip(out_final[\"x_smooth\"].to_numpy(), 0.0, 120.0)\n    y_final = np.clip(out_final[\"y_smooth\"].to_numpy(), 0.0, 53.3)\n\n    # Return as Polars DataFrame in original order\n    return pl.DataFrame({\"x\": x_final, \"y\": y_final})\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code]\n# ======================== MAIN (INFERENCE ONLY) ==========================\nif __name__ == \"__main__\":\n    # 1) Load pretrained LGBM models from MODELS_DIR\n    load_lgbm_models()\n\n    # 2) Run Kaggle evaluation server if available\n    if HAS_EVAL_SERVER and NFLInferenceServer is not None:\n        inference_server = NFLInferenceServer(predict)\n\n        # On Kaggle's actual scoring runs\n        if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n            inference_server.serve()\n        else:\n            # Local gateway on public mock test to create submission.csv\n            print(\"\\n[LOCAL] Running local gateway to generate submission.csv...\")\n            inference_server.run_local_gateway((DATA_DIR,))\n            print(\"✓ submission.csv should now be in the working directory\")\n    else:\n        print(\n            \"\\nNFLInferenceServer is not available (kaggle_evaluation not found). \"\n            \"Training & debugging is fine, but to generate a real submission.csv \"\n            \"you must run this notebook on Kaggle with the competition data attached.\"\n        )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}